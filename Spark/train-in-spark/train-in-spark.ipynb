{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/training/train-in-spark/train-in-spark.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 05. Train in Spark\n",
        "* Create Workspace\n",
        "* Create Experiment\n",
        "* Copy relevant files to the script folder\n",
        "* Configure and Run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't already to establish your connection to the AzureML Workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check core SDK version number\n",
        "import azureml.core\n",
        "\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDK version: 1.13.0\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1613516126165
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Workspace\n",
        "\n",
        "Initialize a workspace object from persisted configuration."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zhenzhuUKSouth\n",
            "zhenzhuUKSouth\n",
            "uksouth\n",
            "e9b2ec51-5c94-4fa8-809a-dc1e695e4896\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1613612772768
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Experiment\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = 'train-on-spark-mmlspark'\n",
        "\n",
        "from azureml.core import Experiment\n",
        "exp = Experiment(workspace=ws, name=experiment_name)"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1613612773408
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View `train-spark.py`\n",
        "\n",
        "For convenience, we created a training script for you. It is printed below as a text, but you can also run `%pfile ./train-spark.py` in a cell to show the file."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "script = 'Classification - Before and After MMLSpark.py'\n",
        "with open(script, 'r') as training_script:\n",
        "    print(training_script.read())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#!/usr/bin/env python\n",
            "# coding: utf-8\n",
            "\n",
            "# ## 103 - Simplifying Machine Learning Pipelines with `mmlspark`\n",
            "# \n",
            "# ### 1. Introduction\n",
            "# \n",
            "# <p><img src=\"https://images-na.ssl-images-amazon.com/images/G/01/img16/books/bookstore/landing-page/1000638_books_landing-page_bookstore-photo-01.jpg\" style=\"width: 500px;\" title=\"Image from https://images-na.ssl-images-amazon.com/images/G/01/img16/books/bookstore/landing-page/1000638_books_landing-page_bookstore-photo-01.jpg\" /><br /></p>\n",
            "# \n",
            "# In this tutorial, we perform the same classification task in two\n",
            "# different ways: once using plain **`pyspark`** and once using the\n",
            "# **`mmlspark`** library.  The two methods yield the same performance,\n",
            "# but one of the two libraries is drastically simpler to use and iterate\n",
            "# on (can you guess which one?).\n",
            "# \n",
            "# The task is simple: Predict whether a user's review of a book sold on\n",
            "# Amazon is good (rating > 3) or bad based on the text of the review.  We\n",
            "# accomplish this by training LogisticRegression learners with different\n",
            "# hyperparameters and choosing the best model.\n",
            "\n",
            "# ### 2. Read the data\n",
            "# \n",
            "# We download and read in the data. We show a sample below:\n",
            "\n",
            "# In[ ]:\n",
            "\n",
            "\n",
            "rawData = spark.read.parquet(\"wasbs://publicwasb@mmlspark.blob.core.windows.net/BookReviewsFromAmazon10K.parquet\")\n",
            "rawData.show(5)\n",
            "\n",
            "\n",
            "# ### 3. Extract more features and process data\n",
            "# \n",
            "# Real data however is more complex than the above dataset. It is common\n",
            "# for a dataset to have features of multiple types: text, numeric,\n",
            "# categorical.  To illustrate how difficult it is to work with these\n",
            "# datasets, we add two numerical features to the dataset: the **word\n",
            "# count** of the review and the **mean word length**.\n",
            "\n",
            "# In[ ]:\n",
            "\n",
            "\n",
            "from pyspark.sql.functions import udf\n",
            "from pyspark.sql.types import *\n",
            "def wordCount(s):\n",
            "    return len(s.split())\n",
            "def wordLength(s):\n",
            "    import numpy as np\n",
            "    ss = [len(w) for w in s.split()]\n",
            "    return round(float(np.mean(ss)), 2)\n",
            "wordLengthUDF = udf(wordLength, DoubleType())\n",
            "wordCountUDF = udf(wordCount, IntegerType())\n",
            "\n",
            "\n",
            "# In[ ]:\n",
            "\n",
            "\n",
            "from mmlspark.stages import UDFTransformer\n",
            "wordLength = \"wordLength\"\n",
            "wordCount = \"wordCount\"\n",
            "wordLengthTransformer = UDFTransformer(inputCol=\"text\", outputCol=wordLength, udf=wordLengthUDF)\n",
            "wordCountTransformer = UDFTransformer(inputCol=\"text\", outputCol=wordCount, udf=wordCountUDF)\n",
            "\n",
            "\n",
            "# In[ ]:\n",
            "\n",
            "\n",
            "from pyspark.ml import Pipeline\n",
            "data = Pipeline(stages=[wordLengthTransformer, wordCountTransformer])        .fit(rawData).transform(rawData)        .withColumn(\"label\", rawData[\"rating\"] > 3).drop(\"rating\")\n",
            "\n",
            "\n",
            "# In[ ]:\n",
            "\n",
            "\n",
            "data.show(5)\n",
            "\n",
            "\n",
            "# ### 4a. Classify using pyspark\n",
            "# \n",
            "# To choose the best LogisticRegression classifier using the `pyspark`\n",
            "# library, need to *explictly* perform the following steps:\n",
            "# \n",
            "# 1. Process the features:\n",
            "#    * Tokenize the text column\n",
            "#    * Hash the tokenized column into a vector using hashing\n",
            "#    * Merge the numeric features with the vector in the step above\n",
            "# 2. Process the label column: cast it into the proper type.\n",
            "# 3. Train multiple LogisticRegression algorithms on the `train` dataset\n",
            "#    with different hyperparameters\n",
            "# 4. Compute the area under the ROC curve for each of the trained models\n",
            "#    and select the model with the highest metric as computed on the\n",
            "#    `test` dataset\n",
            "# 5. Evaluate the best model on the `validation` set\n",
            "# \n",
            "# As you can see below, there is a lot of work involved and a lot of\n",
            "# steps where something can go wrong!\n",
            "\n",
            "# In[ ]:\n",
            "\n",
            "\n",
            "from pyspark.ml.feature import Tokenizer, HashingTF\n",
            "from pyspark.ml.feature import VectorAssembler\n",
            "\n",
            "# Featurize text column\n",
            "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokenizedText\")\n",
            "numFeatures = 10000\n",
            "hashingScheme = HashingTF(inputCol=\"tokenizedText\",\n",
            "                          outputCol=\"TextFeatures\",\n",
            "                          numFeatures=numFeatures)\n",
            "tokenizedData = tokenizer.transform(data)\n",
            "featurizedData = hashingScheme.transform(tokenizedData)\n",
            "\n",
            "# Merge text and numeric features in one feature column\n",
            "featureColumnsArray = [\"TextFeatures\", \"wordCount\", \"wordLength\"]\n",
            "assembler = VectorAssembler(\n",
            "    inputCols = featureColumnsArray,\n",
            "    outputCol=\"features\")\n",
            "assembledData = assembler.transform(featurizedData)\n",
            "\n",
            "# Select only columns of interest\n",
            "# Convert rating column from boolean to int\n",
            "processedData = assembledData                 .select(\"label\", \"features\")                 .withColumn(\"label\", assembledData.label.cast(IntegerType()))\n",
            "\n",
            "\n",
            "# In[ ]:\n",
            "\n",
            "\n",
            "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
            "from pyspark.ml.classification import LogisticRegression\n",
            "\n",
            "# Prepare data for learning\n",
            "train, test, validation = processedData.randomSplit([0.60, 0.20, 0.20], seed=123)\n",
            "\n",
            "# Train the models on the 'train' data\n",
            "lrHyperParams = [0.05, 0.1, 0.2, 0.4]\n",
            "logisticRegressions = [LogisticRegression(regParam = hyperParam)\n",
            "                       for hyperParam in lrHyperParams]\n",
            "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",\n",
            "                                          metricName=\"areaUnderROC\")\n",
            "metrics = []\n",
            "models = []\n",
            "\n",
            "# Select the best model\n",
            "for learner in logisticRegressions:\n",
            "    model = learner.fit(train)\n",
            "    models.append(model)\n",
            "    scoredData = model.transform(test)\n",
            "    metrics.append(evaluator.evaluate(scoredData))\n",
            "bestMetric = max(metrics)\n",
            "bestModel = models[metrics.index(bestMetric)]\n",
            "\n",
            "# Get AUC on the validation dataset\n",
            "scoredVal = bestModel.transform(validation)\n",
            "print(evaluator.evaluate(scoredVal))\n",
            "\n",
            "\n",
            "# ### 4b. Classify using mmlspark\n",
            "# \n",
            "# Life is a lot simpler when using `mmlspark`!\n",
            "# \n",
            "# 1. The **`TrainClassifier`** Estimator featurizes the data internally,\n",
            "#    as long as the columns selected in the `train`, `test`, `validation`\n",
            "#    dataset represent the features\n",
            "# \n",
            "# 2. The **`FindBestModel`** Estimator find the best model from a pool of\n",
            "#    trained models by find the model which performs best on the `test`\n",
            "#    dataset given the specified metric\n",
            "# \n",
            "# 3. The **`CompueModelStatistics`** Transformer computes the different\n",
            "#    metrics on a scored dataset (in our case, the `validation` dataset)\n",
            "#    at the same time\n",
            "\n",
            "# In[ ]:\n",
            "\n",
            "\n",
            "from mmlspark.train import TrainClassifier, ComputeModelStatistics\n",
            "from mmlspark.automl import FindBestModel\n",
            "\n",
            "# Prepare data for learning\n",
            "train, test, validation = data.randomSplit([0.60, 0.20, 0.20], seed=123)\n",
            "\n",
            "# Train the models on the 'train' data\n",
            "lrHyperParams = [0.05, 0.1, 0.2, 0.4]\n",
            "logisticRegressions = [LogisticRegression(regParam = hyperParam)\n",
            "                       for hyperParam in lrHyperParams]\n",
            "lrmodels = [TrainClassifier(model=lrm, labelCol=\"label\", numFeatures=10000).fit(train)\n",
            "            for lrm in logisticRegressions]\n",
            "\n",
            "# Select the best model\n",
            "bestModel = FindBestModel(evaluationMetric=\"AUC\", models=lrmodels).fit(test)\n",
            "\n",
            "\n",
            "# Get AUC on the validation dataset\n",
            "predictions = bestModel.transform(validation)\n",
            "metrics = ComputeModelStatistics().transform(predictions)\n",
            "print(\"Best model's AUC on validation set = \"\n",
            "      + \"{0:.2f}%\".format(metrics.first()[\"AUC\"] * 100))\n",
            "\n",
            "\n"
          ]
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1613612845717
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure & Run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** You can use Docker-based execution to run the Spark job in local computer or a remote VM. Please see the `train-in-remote-vm` notebook for example on how to configure and run in Docker mode in a VM. Make sure you choose a Docker image that has Spark installed, such as `microsoft/mmlspark:0.12`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attach an AML Compute\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\r\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "# Choose a name for your CPU cluster\r\n",
        "cpu_cluster_name = \"spark-low--cpu\"\r\n",
        "\r\n",
        "# Verify that the cluster does not exist already\r\n",
        "try:\r\n",
        "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\r\n",
        "                                                           max_nodes=4, \r\n",
        "                                                           vm_priority=\"lowpriority\",\r\n",
        "                                                           idle_seconds_before_scaledown=2400)\r\n",
        "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\r\n",
        "\r\n",
        "cpu_cluster.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing cluster, use it.\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613612798521
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure HDI run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure an execution using the HDInsight cluster with a conda environment that has `numpy`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.environment import Environment\n",
        "abesparksenv = Environment(name=\"abesparksenv\")\n",
        "# Specify custom Docker base image and registry, if you don't want to use the defaults\n",
        "#abesparksenv.docker.base_image=\"mcr.microsoft.com/mmlspark/release\"\n",
        "abesparksenv.python.user_managed_dependencies = True"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1613612802450
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Alternatively, load the string from a file.\r\n",
        "abesparksenv.docker.base_image = None\r\n",
        "abesparksenv.docker.base_dockerfile = \"./Dockerfile\""
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613612803961
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submit the script to AzureML Compute"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import ScriptRunConfig, Environment\n",
        "\n",
        "script_run_config = ScriptRunConfig(source_directory = '.',\n",
        "                                    script= script,\n",
        "                                    environment=abesparksenv,\n",
        "                                    compute_target=cpu_cluster)\n",
        "run = exp.submit(config=script_run_config)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1613612866962
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monitor the run using a Juypter widget"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "RunDetails(run).show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6b56aad4a9d4de4bb9865ce1ec66113"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/train-on-spark-mmlspark/runs/train-on-spark-mmlspark_1613612863_911b5eb1?wsid=/subscriptions/e9b2ec51-5c94-4fa8-809a-dc1e695e4896/resourcegroups/zhenzhuUKSouth/workspaces/zhenzhuUKSouth\", \"run_id\": \"train-on-spark-mmlspark_1613612863_911b5eb1\", \"run_properties\": {\"run_id\": \"train-on-spark-mmlspark_1613612863_911b5eb1\", \"created_utc\": \"2021-02-18T01:47:46.342248Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"0f8a7196-7d7c-482b-ae4a-aff7e0bad786\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {\"_aml_system_ComputeTargetStatus\": \"{\\\"AllocationState\\\":\\\"steady\\\",\\\"PreparingNodeCount\\\":0,\\\"RunningNodeCount\\\":0,\\\"CurrentNodeCount\\\":1}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2021-02-18T01:48:19.536837Z\", \"status\": \"Failed\", \"log_files\": {\"azureml-logs/55_azureml-execution-tvmps_b58029a3842ad5153ea873b14b77a7b0a9f20556a337d6eea5e6d38dd45b13d8_p.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.train-on-spark-mmlspark_1613612863_911b5eb1/azureml-logs/55_azureml-execution-tvmps_b58029a3842ad5153ea873b14b77a7b0a9f20556a337d6eea5e6d38dd45b13d8_p.txt?sv=2019-02-02&sr=b&sig=PPreUXOmANoUMJWjikneATsqXdeBVIBVYr6E7M3CZA0%3D&st=2021-02-18T01%3A38%3A12Z&se=2021-02-18T09%3A48%3A12Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_b58029a3842ad5153ea873b14b77a7b0a9f20556a337d6eea5e6d38dd45b13d8_p.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.train-on-spark-mmlspark_1613612863_911b5eb1/azureml-logs/65_job_prep-tvmps_b58029a3842ad5153ea873b14b77a7b0a9f20556a337d6eea5e6d38dd45b13d8_p.txt?sv=2019-02-02&sr=b&sig=sOZmrfxDiJLSQRfC1yA0JziO7g6TSVVi84GNaA4cBbI%3D&st=2021-02-18T01%3A38%3A13Z&se=2021-02-18T09%3A48%3A13Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.train-on-spark-mmlspark_1613612863_911b5eb1/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=npS2ol8BHfDkGMKjbrSaqjTiEGVSDsDuXovkq1%2F4kRo%3D&st=2021-02-18T01%3A38%3A13Z&se=2021-02-18T09%3A48%3A13Z&sp=r\", \"azureml-logs/75_job_post-tvmps_b58029a3842ad5153ea873b14b77a7b0a9f20556a337d6eea5e6d38dd45b13d8_p.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.train-on-spark-mmlspark_1613612863_911b5eb1/azureml-logs/75_job_post-tvmps_b58029a3842ad5153ea873b14b77a7b0a9f20556a337d6eea5e6d38dd45b13d8_p.txt?sv=2019-02-02&sr=b&sig=yqPfzcfcizx5D9dmsa2f1jEOX3blmOmO7LN7rh%2BwoKo%3D&st=2021-02-18T01%3A38%3A13Z&se=2021-02-18T09%3A48%3A13Z&sp=r\", \"azureml-logs/process_info.json\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.train-on-spark-mmlspark_1613612863_911b5eb1/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=iuXnbCQ%2Btq1lSpo6HBeqq%2B%2FNnOhdQAKvCgnKOFVO0dk%3D&st=2021-02-18T01%3A38%3A13Z&se=2021-02-18T09%3A48%3A13Z&sp=r\", \"azureml-logs/process_status.json\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.train-on-spark-mmlspark_1613612863_911b5eb1/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=avwDCepkiEastbaP9ohDHRBtTNQTojuPLqTl36HUs5c%3D&st=2021-02-18T01%3A38%3A13Z&se=2021-02-18T09%3A48%3A13Z&sp=r\", \"logs/azureml/64_azureml.log\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.train-on-spark-mmlspark_1613612863_911b5eb1/logs/azureml/64_azureml.log?sv=2019-02-02&sr=b&sig=S0h4ny1tn0vwV%2F6hC6tK95sDJfv3HlOQ6qsEofZy%2F9M%3D&st=2021-02-18T01%3A38%3A12Z&se=2021-02-18T09%3A48%3A12Z&sp=r\", \"logs/azureml/job_prep_azureml.log\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.train-on-spark-mmlspark_1613612863_911b5eb1/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=6EElPOIbXHwKmHfCEm6hveV22OQVkEeUgFoYw5boWXU%3D&st=2021-02-18T01%3A38%3A12Z&se=2021-02-18T09%3A48%3A12Z&sp=r\", \"logs/azureml/job_release_azureml.log\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.train-on-spark-mmlspark_1613612863_911b5eb1/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=q28v2vXcxs%2FK6MZfqnj6Hdsg6ItQaJWoYKMlHhXAW8M%3D&st=2021-02-18T01%3A38%3A12Z&se=2021-02-18T09%3A48%3A12Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\", \"logs/azureml/job_prep_azureml.log\", \"logs/azureml/job_release_azureml.log\"], [\"azureml-logs/55_azureml-execution-tvmps_b58029a3842ad5153ea873b14b77a7b0a9f20556a337d6eea5e6d38dd45b13d8_p.txt\"], [\"logs/azureml/64_azureml.log\"], [\"azureml-logs/65_job_prep-tvmps_b58029a3842ad5153ea873b14b77a7b0a9f20556a337d6eea5e6d38dd45b13d8_p.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"azureml-logs/75_job_post-tvmps_b58029a3842ad5153ea873b14b77a7b0a9f20556a337d6eea5e6d38dd45b13d8_p.txt\"]], \"run_duration\": \"0:00:33\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"[2021-02-18T01:48:08.255711] Entering job release\\r\\n[2021-02-18T01:48:08.946761] Starting job release\\r\\n[2021-02-18T01:48:08.947328] Logging experiment finalizing status in history service.\\r\\nStarting the daemon thread to refresh tokens in background for process with pid = 90\\r\\n[2021-02-18T01:48:08.949357] job release stage : upload_datastore starting...\\r\\n[2021-02-18T01:48:08.949600] job release stage : start importing azureml.history._tracking in run_history_release.\\r\\n[2021-02-18T01:48:08.949911] job release stage : execute_job_release starting...\\r\\n[2021-02-18T01:48:08.950244] job release stage : copy_batchai_cached_logs starting...\\r\\n[2021-02-18T01:48:08.950443] job release stage : copy_batchai_cached_logs completed...\\r\\n[2021-02-18T01:48:08.957857] Entering context manager injector.\\r\\n[2021-02-18T01:48:08.960091] job release stage : upload_datastore completed...\\r\\n[2021-02-18T01:48:09.121250] job release stage : send_run_telemetry starting...\\r\\n[2021-02-18T01:48:09.207059] job release stage : execute_job_release completed...\\r\\n[2021-02-18T01:48:09.792347] job release stage : send_run_telemetry completed...\\r\\n[2021-02-18T01:48:09.792519] Job release is complete\\r\\n\\nError occurred: AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\\tReason: Job failed with non-zero exit Code\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.13.0\"}, \"loading\": false}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1613612871889
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the run is succesfully finished, you can check the metrics logged."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# get all metris logged in the run\n",
        "metrics = run.get_metrics()\n",
        "print(metrics)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Regularization Rate': 0.01, 'Accuracy': 0.9069767441860465}\n"
          ]
        }
      ],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1613519593518
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# register the generated model\n",
        "model = run.register_model(model_name='iris.model', model_path='outputs/iris.model')"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1613519596779
        }
      }
    }
  ],
  "metadata": {
    "index_order": 1,
    "exclude_from_index": false,
    "task": "Submiting a run on a spark cluster",
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "sanpil"
      }
    ],
    "kernel_info": {
      "name": "python3-azureml"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "HDI cluster"
    ],
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "tags": [
      "None"
    ],
    "datasets": [
      "None"
    ],
    "categories": [
      "how-to-use-azureml",
      "training"
    ],
    "category": "training",
    "framework": [
      "PySpark"
    ],
    "friendly_name": "Training in Spark",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}