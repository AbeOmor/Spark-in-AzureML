{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/training/train-in-spark/train-in-spark.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 05. Train in Spark\n",
        "* Create Workspace\n",
        "* Create Experiment\n",
        "* Copy relevant files to the script folder\n",
        "* Configure and Run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't already to establish your connection to the AzureML Workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check core SDK version number\n",
        "import azureml.core\n",
        "\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDK version: 1.13.0\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1613680888471
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Workspace\n",
        "\n",
        "Initialize a workspace object from persisted configuration."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zhenzhuUKSouth\n",
            "zhenzhuUKSouth\n",
            "uksouth\n",
            "e9b2ec51-5c94-4fa8-809a-dc1e695e4896\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1613680893743
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Experiment\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = 'train-on-spark-mmlspark'\n",
        "\n",
        "from azureml.core import Experiment\n",
        "exp = Experiment(workspace=ws, name=experiment_name)"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1613681334121
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View `train-spark.py`\n",
        "\n",
        "For convenience, we created a training script for you. It is printed below as a text, but you can also run `%pfile ./train-spark.py` in a cell to show the file."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "script = 'train-spark.py'\n",
        "with open(script, 'r') as training_script:\n",
        "    print(training_script.read())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Copyright (c) Microsoft. All rights reserved.\n",
            "# Licensed under the MIT license.\n",
            "\n",
            "import numpy as np\n",
            "import pyspark\n",
            "import os\n",
            "import urllib\n",
            "import sys\n",
            "\n",
            "from pyspark.sql.functions import *\n",
            "from pyspark.ml.classification import *\n",
            "from pyspark.ml.evaluation import *\n",
            "from pyspark.ml.feature import *\n",
            "from pyspark.sql.types import StructType, StructField\n",
            "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
            "\n",
            "from azureml.core.run import Run\n",
            "\n",
            "# initialize logger\n",
            "run = Run.get_context()\n",
            "\n",
            "# start Spark session\n",
            "spark = pyspark.sql.SparkSession.builder.appName('Iris').getOrCreate()\n",
            "\n",
            "# print runtime versions\n",
            "print('****************')\n",
            "print('Python version: {}'.format(sys.version))\n",
            "print('Spark version: {}'.format(spark.version))\n",
            "print('****************')\n",
            "\n",
            "# load iris.csv into Spark dataframe\n",
            "schema = StructType([\n",
            "    StructField(\"sepal-length\", DoubleType()),\n",
            "    StructField(\"sepal-width\", DoubleType()),\n",
            "    StructField(\"petal-length\", DoubleType()),\n",
            "    StructField(\"petal-width\", DoubleType()),\n",
            "    StructField(\"class\", StringType())\n",
            "])\n",
            "\n",
            "data = spark.read.format(\"com.databricks.spark.csv\") \\\n",
            "    .option(\"header\", \"true\") \\\n",
            "    .schema(schema) \\\n",
            "    .load(\"iris.csv\")\n",
            "\n",
            "print(\"First 10 rows of Iris dataset:\")\n",
            "data.show(10)\n",
            "\n",
            "# vectorize all numerical columns into a single feature column\n",
            "feature_cols = data.columns[:-1]\n",
            "assembler = pyspark.ml.feature.VectorAssembler(\n",
            "    inputCols=feature_cols, outputCol='features')\n",
            "data = assembler.transform(data)\n",
            "\n",
            "# convert text labels into indices\n",
            "data = data.select(['features', 'class'])\n",
            "label_indexer = pyspark.ml.feature.StringIndexer(\n",
            "    inputCol='class', outputCol='label').fit(data)\n",
            "data = label_indexer.transform(data)\n",
            "\n",
            "# only select the features and label column\n",
            "data = data.select(['features', 'label'])\n",
            "print(\"Reading for machine learning\")\n",
            "data.show(10)\n",
            "\n",
            "# change regularization rate and you will likely get a different accuracy.\n",
            "reg = 0.01\n",
            "# load regularization rate from argument if present\n",
            "if len(sys.argv) > 1:\n",
            "    reg = float(sys.argv[1])\n",
            "\n",
            "# log regularization rate\n",
            "run.log(\"Regularization Rate\", reg)\n",
            "\n",
            "# use Logistic Regression to train on the training set\n",
            "train, test = data.randomSplit([0.70, 0.30])\n",
            "lr = pyspark.ml.classification.LogisticRegression(regParam=reg)\n",
            "model = lr.fit(train)\n",
            "\n",
            "model.save(os.path.join(\"outputs\", \"iris.model\"))\n",
            "\n",
            "# predict on the test set\n",
            "prediction = model.transform(test)\n",
            "print(\"Prediction\")\n",
            "prediction.show(10)\n",
            "\n",
            "# evaluate the accuracy of the model using the test set\n",
            "evaluator = pyspark.ml.evaluation.MulticlassClassificationEvaluator(\n",
            "    metricName='accuracy')\n",
            "accuracy = evaluator.evaluate(prediction)\n",
            "\n",
            "print()\n",
            "print('#####################################')\n",
            "print('Regularization rate is {}'.format(reg))\n",
            "print(\"Accuracy is {}\".format(accuracy))\n",
            "print('#####################################')\n",
            "print()\n",
            "\n",
            "# log accuracy\n",
            "run.log('Accuracy', accuracy)\n",
            "\n"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1613681336857
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure & Run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** You can use Docker-based execution to run the Spark job in local computer or a remote VM. Please see the `train-in-remote-vm` notebook for example on how to configure and run in Docker mode in a VM. Make sure you choose a Docker image that has Spark installed, such as `microsoft/mmlspark:0.12`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attach an AML Compute\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\r\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "# Choose a name for your CPU cluster\r\n",
        "cpu_cluster_name = \"spark-low--cpu\"\r\n",
        "\r\n",
        "# Verify that the cluster does not exist already\r\n",
        "try:\r\n",
        "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\r\n",
        "                                                           max_nodes=4, \r\n",
        "                                                           vm_priority=\"lowpriority\",\r\n",
        "                                                           idle_seconds_before_scaledown=2400)\r\n",
        "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\r\n",
        "\r\n",
        "cpu_cluster.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing cluster, use it.\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613680899787
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure AzureML Environment run and Custom Docker"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure AzureML custom environment to use [mmlspark](https://mmlspark.blob.core.windows.net/website/index.html#install) docker image."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.environment import Environment\n",
        "abesparksenv = Environment(name=\"abesparksenv\")\n",
        "# Specify custom Docker base image and registry, if you don't want to use the defaults\n",
        "#abesparksenv.docker.base_image=\"mcr.microsoft.com/mmlspark/release\"\n",
        "abesparksenv.python.user_managed_dependencies = True"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1613681009654
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choose the base image\r\n",
        "Most Dockerfiles start from a parent image, rather than from scratch. Azure ML has a set of maintained images that serve as base images for training and inference. You can find information on these images, including the Dockerfiles used to build them, at this GitHub repo [Azure/AzureML-Containers](https://github.com/Azure/AzureML-Containers). Depending on your scenario, you should pick an appropriate CPU or GPU image to use as the parent image. The base images include Miniconda, and the GPU base images include the necessary GPU drivers needed to run GPU jobs on Azure ML. [Here](https://github.com/Azure/AzureML-Containers#featured-tags) is the list of images and associated tags for the Azure ML base images.\r\n",
        "\r\n",
        "As an example, we will use one of the mmlspark images as the parent image for our Dockerfile:\r\n",
        "\r\n",
        "```\r\n",
        "FROM mcr.microsoft.com/mmlspark/release\r\n",
        "```\r\n",
        "\r\n",
        "### Specify conda dependencies\r\n",
        "Since using Azure ML has some Python dependencies, we will add an instruction to install these dependencies via Conda, an open-source package management system:\r\n",
        "\r\n",
        "1. **azureml-defaults**, a lightweight version of the full Azure ML Python SDK that includes the `azureml-core` and `applicationinsights` packages required for tasks such as logging metrics, uploading artifacts, accessing datastores from within runs, and inference. `azureml-defaults` should be sufficient for most remote training and deployment scenarios; if for some reason you need the full SDK, you can specify pip installing the full `azureml-sdk` package instead.\r\n",
        "\r\n",
        "```\r\n",
        "FROM mcr.microsoft.com/mmlspark/release\r\n",
        "RUN conda install -y pip=20.1.1 && \\\r\n",
        "    conda clean -ay && \\\r\n",
        "    pip install --no-cache-dir azureml-defaults \\\r\n",
        "    pip install azureml-core\r\n",
        "```\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Alternatively, load the string from a file.\r\n",
        "abesparksenv.docker.base_image = None\r\n",
        "abesparksenv.docker.base_dockerfile = \"./Dockerfile\""
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613681012319
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submit the script to AzureML Compute"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import ScriptRunConfig\n",
        "\n",
        "script_run_config = ScriptRunConfig(source_directory = '.',\n",
        "                                    script= script,\n",
        "                                    environment=abesparksenv,\n",
        "                                    compute_target=cpu_cluster)\n",
        "run = exp.submit(config=script_run_config)"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1613681882294
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monitor the run using a Juypter widget"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "RunDetails(run).show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613612871889
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the run is succesfully finished, you can check the metrics logged."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# get all metris logged in the run\n",
        "metrics = run.get_metrics()\n",
        "print(metrics)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613519593518
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# register the generated model\n",
        "model = run.register_model(model_name='iris.model', model_path='outputs/iris.model')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613519596779
        }
      }
    }
  ],
  "metadata": {
    "index_order": 1,
    "exclude_from_index": false,
    "task": "Submiting a run on a spark cluster",
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "sanpil"
      }
    ],
    "kernel_info": {
      "name": "python3-azureml"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "HDI cluster"
    ],
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "tags": [
      "None"
    ],
    "datasets": [
      "None"
    ],
    "categories": [
      "how-to-use-azureml",
      "training"
    ],
    "category": "training",
    "framework": [
      "PySpark"
    ],
    "friendly_name": "Training in Spark",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}