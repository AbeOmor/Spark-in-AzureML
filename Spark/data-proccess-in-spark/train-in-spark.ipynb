{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/training/train-in-spark/train-in-spark.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 05. Train in Spark\n",
        "* Create Workspace\n",
        "* Create Experiment\n",
        "* Copy relevant files to the script folder\n",
        "* Configure and Run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't already to establish your connection to the AzureML Workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check core SDK version number\n",
        "import azureml.core\n",
        "\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDK version: 1.22.0\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1613712328536
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Workspace\n",
        "\n",
        "Initialize a workspace object from persisted configuration."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zhenzhuuksouth\n",
            "zhenzhuuksouth\n",
            "uksouth\n",
            "e9b2ec51-5c94-4fa8-809a-dc1e695e4896\n"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1613712332849
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Experiment\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = 'data-processing-on-spark'\n",
        "\n",
        "from azureml.core import Experiment\n",
        "exp = Experiment(workspace=ws, name=experiment_name)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1613712333038
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View `train-spark.py`\n",
        "\n",
        "For convenience, we created a training script for you. It is printed below as a text, but you can also run `%pfile ./train-spark.py` in a cell to show the file."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "with open('stackoverflow-data-prep.py', 'r') as training_script:\n",
        "    print(training_script.read())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#!/usr/bin/env python\n",
            "# coding: utf-8\n",
            "\n",
            "# Copyright (c) Microsoft. All rights reserved.\n",
            "# Licensed under the MIT license.\n",
            "\n",
            "import numpy as np\n",
            "import pyspark\n",
            "import os\n",
            "import urllib\n",
            "import sys\n",
            "\n",
            "from pyspark.sql.functions import *\n",
            "from pyspark.ml.classification import *\n",
            "from pyspark.ml.evaluation import *\n",
            "from pyspark.ml.feature import *\n",
            "from pyspark.sql.types import StructType, StructField\n",
            "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
            "\n",
            "from azureml.core.run import Run\n",
            "\n",
            "# initialize logger\n",
            "run = Run.get_context()\n",
            "\n",
            "# start Spark session\n",
            "spark = pyspark.sql.SparkSession.builder.appName('Stackoverflow') \\\n",
            "            .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.11:0.6.0\") \\\n",
            "            .config(\"spark.jars.repositories\", \"https://mvnrepository.com/artifact/com.databricks/spark-xml\") \\\n",
            "            .getOrCreate()\n",
            "\n",
            "# print runtime versions\n",
            "print('****************')\n",
            "print('Python version: {}'.format(sys.version))\n",
            "print('Spark version: {}'.format(spark.version))\n",
            "print('****************')\n",
            "'''\n",
            "STEP 1: Download Stack Overflow data from archive. (This takes about 2-3 hours)\n",
            "'''\n",
            "# get_ipython().system('wget https://archive.org/download/stackexchange/stackoverflow.com-Posts.7z')\n",
            "# get_ipython().system('sudo apt-get install p7zip-full')\n",
            "# get_ipython().system('7z x stackoverflow.com-Posts.7z -oposts')\n",
            "\n",
            "# Define input arguments\n",
            "import sys\n",
            "data_dir = sys.argv[1]\n",
            "#data_dir = '../../../../../stackoverflow'\n",
            "xml_file_path = str(os.path.join(data_dir, 'stackoverflow/posts/Posts.xml'))\n",
            "#xml_file_path = 'wasbs://zhenzhuuksouth3632161177.blob.core.windows.net/stackoverflowdata/stackoverflow/posts/Posts.xml'\n",
            "print('*******TESTING PATHS FOR DATA*********')\n",
            "print(xml_file_path)\n",
            "print(os.listdir(str(os.path.join(data_dir, 'stackoverflow'))))\n",
            "print('****************')\n",
            "\n",
            "'''\n",
            "STEP 2: Copy data over to databricks file system (This takes about an hour)\n",
            "'''\n",
            "## TODO Need to figure out how to move data to to a mounted Hadoop Storage for the AML Cluster\n",
            "#dbutils.fs.cp(xml_file_path, \"dbfs:/tmp/posts/Posts.xml\")  \n",
            "\n",
            "'''\n",
            "STEP 3: Process data using Spark\n",
            "\n",
            "Note - This requires the spark-xml maven library (com.databricks:spark-xml_2.11:0.6.0) to be installed.\n",
            "'''\n",
            "import pyspark\n",
            "from pyspark.sql import functions as sf\n",
            "from pyspark.sql import Row\n",
            "from pyspark.sql.functions import size, col, concat_ws, rtrim, regexp_replace, split, udf\n",
            "from pyspark.sql.types import ArrayType\n",
            "\n",
            "# load xml file into spark data frame.\n",
            "posts = spark.read.format(\"xml\").option(\"rowTag\", \"row\").load(xml_file_path)\n",
            "\n",
            "# select only questions\n",
            "questions = posts.filter(posts._PostTypeId == 1) \n",
            "\n",
            "# drop irrelvant columns and clean up strings\n",
            "questions = questions.select([c for c in questions.columns if c in ['_Id','_Title','_Body','_Tags']])\n",
            "questions = questions.withColumn('full_question', sf.concat(sf.col('_Title'), sf.lit(' '), sf.col('_Body')))\n",
            "questions = questions.select([c for c in questions.columns if c in ['_Id','full_question','_Tags']]).withColumn(\"full_question\", regexp_replace(\"full_question\", \"[\\\\n,]\", \" \"))\n",
            "questions = questions.withColumn(\"_Tags\", regexp_replace(\"_Tags\", \"><\", \" \"))\n",
            "questions = questions.withColumn(\"_Tags\", regexp_replace(\"_Tags\", \"(>|<)\", \"\"))\n",
            "questions = questions.withColumn('_Tags', rtrim(questions._Tags))\n",
            "questions = questions.withColumn('_Tags', split(questions._Tags, \" \"))\n",
            "\n",
            "# filter out to single tags in following list\n",
            "tags_of_interest = ['azure-devops', 'azure-functions', 'azure-web-app-service', 'azure-storage', 'azure-virtual-machine'] \n",
            "\n",
            "def intersect(xs):\n",
            "    xs = set(xs)\n",
            "    @udf(\"array<string>\")\n",
            "    def _(ys):\n",
            "        return list(xs.intersection(ys))\n",
            "    return _\n",
            "\n",
            "questions = questions.withColumn(\"intersect\", intersect(tags_of_interest)(\"_Tags\"))\n",
            "questions = questions.filter(size(col(\"intersect\"))==1)\n",
            "questions = questions.select('_Id', 'full_question', 'intersect').withColumn('_Tags', concat_ws(', ', 'intersect'))\n",
            "questions = questions.select('_Id', 'full_question', '_Tags')\n",
            "\n",
            "questions.show()\n",
            "\n",
            "\n",
            "'''\n",
            "Step 4: Convert processed data into pandas data frame for final preprocessing and data split\n",
            "'''\n",
            "\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "from sklearn.utils import shuffle\n",
            "\n",
            "df = questions.toPandas()\n",
            "\n",
            "# drop nan values and remove line breaks\n",
            "df.dropna(inplace=True)\n",
            "df.replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
            "\n",
            "# balance dataset \n",
            "balanced = df.groupby('_Tags')\n",
            "balanced.apply(lambda x: x.sample(balanced.size().min())).reset_index(drop=True).to_csv('balanced.csv')\n",
            "bd = pd.read_csv('balanced.csv')\n",
            "bd.drop('Unnamed: 0', axis=1, inplace=True)\n",
            "\n",
            "# shuffle data \n",
            "bd = shuffle(bd)\n",
            "\n",
            "# split data into train, test, and valid sets\n",
            "msk = np.random.rand(len(bd)) < 0.7\n",
            "train = bd[msk]\n",
            "temp = bd[~msk]\n",
            "msk = np.random.rand(len(temp)) < 0.66\n",
            "valid = temp[msk]\n",
            "test = temp[~msk]\n",
            "\n",
            "'''\n",
            "STEP 5: Save dataset into csv and class.txt files\n",
            "'''\n",
            "\n",
            "output_dir = './output'\n",
            "\n",
            "import os\n",
            "if not os.path.exists(output_dir):\n",
            "    os.makedirs(output_dir)\n",
            "\n",
            "# create and save classes.txt file\n",
            "classes = pd.DataFrame(bd['_Tags'].unique().tolist())\n",
            "classes.to_csv(os.path.join(output_dir, 'classes.txt'), header=False, index=False)\n",
            "\n",
            "# save train, valid, and test files\n",
            "train.to_csv(os.path.join(output_dir, 'train.csv'), header=False, index=False)\n",
            "valid.to_csv(os.path.join(output_dir, 'valid.csv'), header=False, index=False)\n",
            "test.to_csv(os.path.join(output_dir, 'test.csv'), header=False, index=False)\n",
            "\n"
          ]
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1613712334319
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure & Run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** You can use Docker-based execution to run the Spark job in local computer or a remote VM. Please see the `train-in-remote-vm` notebook for example on how to configure and run in Docker mode in a VM. Make sure you choose a Docker image that has Spark installed, such as `microsoft/mmlspark:0.12`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attach an AML Compute\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\r\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "# Choose a name for your CPU cluster\r\n",
        "cpu_cluster_name = \"spark-data-proc\"\r\n",
        "\r\n",
        "# Verify that the cluster does not exist already\r\n",
        "try:\r\n",
        "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D12_V2',\r\n",
        "                                                           max_nodes=4, \r\n",
        "                                                           vm_priority=\"lowpriority\",\r\n",
        "                                                           idle_seconds_before_scaledown=2400)\r\n",
        "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\r\n",
        "\r\n",
        "cpu_cluster.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing cluster, use it.\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613712339034
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure HDI run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure an execution using the HDInsight cluster with a conda environment that has `numpy`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.environment import Environment\n",
        "abesparksenv = Environment(name=\"abesparksenv\")\n",
        "# Specify custom Docker base image and registry, if you don't want to use the defaults\n",
        "#abesparksenv.docker.base_image=\"mcr.microsoft.com/mmlspark/release\"\n",
        "abesparksenv.python.user_managed_dependencies = True"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1613712343836
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Alternatively, load the string from a file.\r\n",
        "abesparksenv.docker.base_image = None\r\n",
        "abesparksenv.docker.base_dockerfile = \"./Dockerfile\""
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613718436660
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Datastore and DataRef for Spark to talk to"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TODO, write instruction on how to mount to a Blob datastore"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Dataset\r\n",
        "stackoverflow_dataset = ws.datasets['stackoverflowforSpark']\r\n",
        "\r\n",
        "stackoverflow_datastore = ws.datastores['stackoverflow_blob']\r\n",
        "data_ref = stackoverflow_datastore.as_mount()"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1613718582610
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submit the script to AzureML Compute"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import ScriptRunConfig, Environment\n",
        "\n",
        "script_run_config = ScriptRunConfig(source_directory = '.',\n",
        "                                    script= 'stackoverflow-data-prep.py',\n",
        "                                    arguments=[str(data_ref)],\n",
        "                                    environment=abesparksenv,\n",
        "                                    compute_target=cpu_cluster)\n",
        "script_run_config.run_config.data_references[data_ref.data_reference_name] = data_ref.to_config()\n",
        "run = exp.submit(config=script_run_config)\n"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1613718590940
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monitor the run using a Juypter widget"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "RunDetails(run).show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acce88ca05e540ac86140d28a559c629"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/data-processing-on-spark/runs/data-processing-on-spark_1613718585_bd56fbca?wsid=/subscriptions/e9b2ec51-5c94-4fa8-809a-dc1e695e4896/resourcegroups/zhenzhuuksouth/workspaces/zhenzhuuksouth\", \"run_id\": \"data-processing-on-spark_1613718585_bd56fbca\", \"run_properties\": {\"run_id\": \"data-processing-on-spark_1613718585_bd56fbca\", \"created_utc\": \"2021-02-19T07:09:50.047443Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"21de5b57-0e43-4bc3-8b21-b6f68e99a6b2\", \"azureml.git.repository_uri\": \"https://github.com/AbeOmor/Spark-in-AzureML.git\", \"mlflow.source.git.repoURL\": \"https://github.com/AbeOmor/Spark-in-AzureML.git\", \"azureml.git.branch\": \"main\", \"mlflow.source.git.branch\": \"main\", \"azureml.git.commit\": \"fda2e6d559a620ef27f6255f52c13d8f0482c524\", \"mlflow.source.git.commit\": \"fda2e6d559a620ef27f6255f52c13d8f0482c524\", \"azureml.git.dirty\": \"True\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2021-02-19T07:10:09.32694Z\", \"status\": \"Failed\", \"log_files\": {\"azureml-logs/20_image_build_log.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1613718585_bd56fbca/azureml-logs/20_image_build_log.txt?sv=2019-02-02&sr=b&sig=CIr601WQ4jXDop3mwf1%2F6YyGvrTHZFSZlUtfdGydAJ0%3D&st=2021-02-19T06%3A59%3A52Z&se=2021-02-19T15%3A09%3A52Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/20_image_build_log.txt\"]], \"run_duration\": \"0:00:19\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"2021/02/19 07:07:34 Downloading source code...\\r\\n2021/02/19 07:07:35 Finished downloading source code\\r\\n2021/02/19 07:07:36 Creating Docker network: acb_default_network, driver: 'bridge'\\n2021/02/19 07:07:37 Successfully set up Docker network: acb_default_network\\n2021/02/19 07:07:37 Setting up Docker configuration...\\n2021/02/19 07:07:38 Successfully set up Docker configuration\\n2021/02/19 07:07:38 Logging in to registry: zhenzhuuksoudbe914b0.azurecr.io\\r\\n2021/02/19 07:07:39 Successfully logged into zhenzhuuksoudbe914b0.azurecr.io\\n2021/02/19 07:07:39 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\\n2021/02/19 07:07:39 Scanning for dependencies...\\n2021/02/19 07:07:41 Successfully scanned dependencies\\n2021/02/19 07:07:41 Launching container with name: acb_step_0\\r\\nSending build context to Docker daemon  64.51kB\\r\\r\\nStep 1/10 : FROM mcr.microsoft.com/mmlspark/release\\nlatest: Pulling from mmlspark/release\\n4f53fa4d2cf0: Pulling fs layer\\n6af7c939e38e: Pulling fs layer\\n903d0ffd64f6: Pulling fs layer\\n04feeed388b7: Pulling fs layer\\n5adbd8b05e33: Pulling fs layer\\ne03d67bcb2b9: Pulling fs layer\\n84e084395855: Pulling fs layer\\na332f24597fa: Pulling fs layer\\n04feeed388b7: Waiting\\n5adbd8b05e33: Waiting\\ne03d67bcb2b9: Waiting\\n84e084395855: Waiting\\na332f24597fa: Waiting\\n6af7c939e38e: Download complete\\n903d0ffd64f6: Verifying Checksum\\n903d0ffd64f6: Download complete\\n04feeed388b7: Verifying Checksum\\n04feeed388b7: Download complete\\n4f53fa4d2cf0: Verifying Checksum\\n4f53fa4d2cf0: Download complete\\r\\n84e084395855: Verifying Checksum\\n84e084395855: Download complete\\na332f24597fa: Verifying Checksum\\na332f24597fa: Download complete\\n4f53fa4d2cf0: Pull complete\\r\\n6af7c939e38e: Pull complete\\n903d0ffd64f6: Pull complete\\n04feeed388b7: Pull complete\\ne03d67bcb2b9: Verifying Checksum\\ne03d67bcb2b9: Download complete\\r\\n5adbd8b05e33: Verifying Checksum\\n5adbd8b05e33: Download complete\\r\\n5adbd8b05e33: Pull complete\\r\\ne03d67bcb2b9: Pull complete\\r\\n84e084395855: Pull complete\\na332f24597fa: Pull complete\\nDigest: sha256:b391b99e8d753f93b0f9828fc0517b002176232a2adcd04e6e3626b381916738\\nStatus: Downloaded newer image for mcr.microsoft.com/mmlspark/release:latest\\n ---> 015f87568562\\nStep 2/10 : RUN conda install -y pip=20.1.1 &&     conda clean -ay &&     pip install --no-cache-dir azureml-defaults     pip install azureml-core     pip install absl-py     pip install pip install -U scikit-learn\\n ---> Running in 7da32657d1c1\\nCollecting package metadata (current_repodata.json): ...working... done\\r\\nSolving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\\nCollecting package metadata (repodata.json): ...working... done\\r\\nSolving environment: ...working... \\r\\ndone\\n\\n## Package Plan ##\\n\\n  environment location: /usr/local\\n\\n  added / updated specs:\\n    - pip=20.1.1\\n\\n\\nThe following packages will be downloaded:\\n\\n    package                    |            build\\n    ---------------------------|-----------------\\n    ca-certificates-2021.1.19  |       h06a4308_0         121 KB\\n    certifi-2020.12.5          |   py37h06a4308_0         141 KB\\n    conda-4.9.2                |   py37h06a4308_0         2.9 MB\\n    openssl-1.1.1j             |       h27cfd23_0         2.5 MB\\n    pip-20.1.1                 |           py37_1         1.7 MB\\n    ------------------------------------------------------------\\n                                           Total:         7.4 MB\\n\\nThe following packages will be UPDATED:\\n\\n  ca-certificates                               2020.7.22-0 --> 2021.1.19-h06a4308_0\\n  certifi                                  2020.6.20-py37_0 --> 2020.12.5-py37h06a4308_0\\n  conda                                        4.8.5-py37_0 --> 4.9.2-py37h06a4308_0\\n  openssl                                 1.1.1h-h7b6447c_0 --> 1.1.1j-h27cfd23_0\\n\\nThe following packages will be DOWNGRADED:\\n\\n  pip                                         20.2.3-py37_0 --> 20.1.1-py37_1\\n\\n\\n\\nDownloading and Extracting Packages\\n\\rcertifi-2020.12.5    | 141 KB    |            |   0% \\rcertifi-2020.12.5    | 141 KB    | ########## | 100% \\n\\ropenssl-1.1.1j       | 2.5 MB    |            |   0% \\ropenssl-1.1.1j       | 2.5 MB    | ########## | 100% \\ropenssl-1.1.1j       | 2.5 MB    | ########## | 100% \\n\\rconda-4.9.2          | 2.9 MB    |            |   0% \\rconda-4.9.2          | 2.9 MB    | ########## | 100% \\rconda-4.9.2          | 2.9 MB    | ########## | 100% \\n\\rca-certificates-2021 | 121 KB    |            |   0% \\rca-certificates-2021 | 121 KB    | ########## | 100% \\n\\rpip-20.1.1           | 1.7 MB    |            |   0% \\rpip-20.1.1           | 1.7 MB    | ########## | 100% \\rpip-20.1.1           | 1.7 MB    | ########## | 100% \\nPreparing transaction: ...working... done\\nVerifying transaction: ...working... done\\nExecuting transaction: ...working... done\\r\\nCache location: /usr/local/pkgs\\nWill remove the following tarballs:\\n\\n/usr/local/pkgs\\n---------------\\nca-certificates-2021.1.19-h06a4308_0.conda     121 KB\\npip-20.1.1-py37_1.conda                      1.7 MB\\nconda-4.9.2-py37h06a4308_0.conda             2.9 MB\\nopenssl-1.1.1j-h27cfd23_0.conda              2.5 MB\\ncertifi-2020.12.5-py37h06a4308_0.conda       141 KB\\n\\n---------------------------------------------------\\nTotal:                                       7.4 MB\\n\\nRemoved ca-certificates-2021.1.19-h06a4308_0.conda\\nRemoved pip-20.1.1-py37_1.conda\\nRemoved conda-4.9.2-py37h06a4308_0.conda\\nRemoved openssl-1.1.1j-h27cfd23_0.conda\\nRemoved certifi-2020.12.5-py37h06a4308_0.conda\\nWARNING: /root/.conda/pkgs does not exist\\nCache location: \\nThere are no unused packages to remove\\nCollecting azureml-defaults\\r\\n  Downloading azureml_defaults-1.22.0-py3-none-any.whl (3.1 kB)\\nCollecting pip\\n  Downloading pip-21.0.1-py3-none-any.whl (1.5 MB)\\nCollecting install\\n  Downloading install-1.3.4-py3-none-any.whl (3.1 kB)\\nCollecting azureml-core\\n  Downloading azureml_core-1.22.0-py3-none-any.whl (2.1 MB)\\nCollecting absl-py\\n  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\\nCollecting scikit-learn\\n  Downloading scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\\nCollecting azureml-dataset-runtime[fuse]~=1.22.0\\r\\n  Downloading azureml_dataset_runtime-1.22.0-py3-none-any.whl (3.4 kB)\\nCollecting configparser==3.7.4\\n  Downloading configparser-3.7.4-py2.py3-none-any.whl (22 kB)\\nCollecting applicationinsights>=0.11.7\\n  Downloading applicationinsights-0.11.9-py2.py3-none-any.whl (58 kB)\\nCollecting azureml-model-management-sdk==1.0.1b6.post1\\n  Downloading azureml_model_management_sdk-1.0.1b6.post1-py2.py3-none-any.whl (130 kB)\\nCollecting flask==1.0.3\\n  Downloading Flask-1.0.3-py2.py3-none-any.whl (92 kB)\\nCollecting gunicorn==19.9.0\\n  Downloading gunicorn-19.9.0-py2.py3-none-any.whl (112 kB)\\nCollecting json-logging-py==0.2\\n  Downloading json-logging-py-0.2.tar.gz (3.6 kB)\\nCollecting werkzeug<=1.0.1,>=0.16.1\\n  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\\nCollecting azure-mgmt-storage<16.0.0,>=1.5.0\\n  Downloading azure_mgmt_storage-11.2.0-py2.py3-none-any.whl (547 kB)\\nCollecting azure-graphrbac<1.0.0,>=0.40.0\\n  Downloading azure_graphrbac-0.61.1-py2.py3-none-any.whl (141 kB)\\nCollecting msrestazure>=0.4.33\\n  Downloading msrestazure-0.6.4-py2.py3-none-any.whl (40 kB)\\nCollecting jsonpickle\\n  Downloading jsonpickle-2.0.0-py2.py3-none-any.whl (37 kB)\\nCollecting azure-mgmt-resource<15.0.0,>=1.2.1\\n  Downloading azure_mgmt_resource-12.0.0-py2.py3-none-any.whl (1.1 MB)\\nCollecting jmespath\\n  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\\nCollecting pathspec\\n  Downloading pathspec-0.8.1-py2.py3-none-any.whl (28 kB)\\nRequirement already satisfied, skipping upgrade: requests>=2.19.1 in /usr/local/lib/python3.7/site-packages (from azureml-core) (2.24.0)\\nCollecting azure-common>=1.1.12\\n  Downloading azure_common-1.1.26-py2.py3-none-any.whl (12 kB)\\nCollecting PyJWT<3.0.0\\n  Downloading PyJWT-2.0.1-py3-none-any.whl (15 kB)\\nCollecting backports.tempfile\\n  Downloading backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB)\\nCollecting SecretStorage\\r\\n  Downloading SecretStorage-3.3.1-py3-none-any.whl (15 kB)\\nCollecting azure-mgmt-containerregistry>=2.0.0\\n  Downloading azure_mgmt_containerregistry-2.8.0-py2.py3-none-any.whl (718 kB)\\nRequirement already satisfied, skipping upgrade: urllib3>=1.23 in /usr/local/lib/python3.7/site-packages (from azureml-core) (1.25.10)\\nRequirement already satisfied, skipping upgrade: cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.* in /usr/local/lib/python3.7/site-packages (from azureml-core) (3.1.1)\\nRequirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.7/site-packages (from azureml-core) (2020.1)\\nCollecting azure-mgmt-authorization<1.0.0,>=0.40.0\\n  Downloading azure_mgmt_authorization-0.61.0-py2.py3-none-any.whl (94 kB)\\nCollecting contextlib2\\n  Downloading contextlib2-0.6.0.post1-py2.py3-none-any.whl (9.8 kB)\\nCollecting ndg-httpsclient\\n  Downloading ndg_httpsclient-0.5.1-py3-none-any.whl (34 kB)\\nRequirement already satisfied, skipping upgrade: pyopenssl<21.0.0 in /usr/local/lib/python3.7/site-packages (from azureml-core) (19.1.0)\\nCollecting docker\\n  Downloading docker-4.4.3-py2.py3-none-any.whl (146 kB)\\nCollecting ruamel.yaml>=0.15.35\\n  Downloading ruamel.yaml-0.16.12-py2.py3-none-any.whl (111 kB)\\nCollecting msrest>=0.5.1\\n  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\\nCollecting azure-mgmt-keyvault<7.0.0,>=0.40.0\\n  Downloading azure_mgmt_keyvault-2.2.0-py2.py3-none-any.whl (89 kB)\\nCollecting adal>=1.2.0\\n  Downloading adal-1.2.6-py2.py3-none-any.whl (55 kB)\\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/site-packages (from azureml-core) (2.8.1)\\nRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/site-packages (from absl-py) (1.15.0)\\nCollecting joblib>=0.11\\n  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\\nCollecting threadpoolctl>=2.0.0\\n  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\\nRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/site-packages (from scikit-learn) (1.19.1)\\nCollecting scipy>=0.19.1\\r\\n  Downloading scipy-1.6.1-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\\nCollecting pyarrow<2.0.0,>=0.17.0\\n  Downloading pyarrow-1.0.1-cp37-cp37m-manylinux2014_x86_64.whl (17.3 MB)\\nCollecting azureml-dataprep<2.10.0a,>=2.9.0a\\r\\n  Downloading azureml_dataprep-2.9.1-py3-none-any.whl (39.4 MB)\\nCollecting fusepy<4.0.0,>=3.0.1; extra == \\\"fuse\\\"\\n  Downloading fusepy-3.0.1.tar.gz (11 kB)\\nCollecting liac-arff>=2.1.1\\n  Downloading liac-arff-2.5.0.tar.gz (13 kB)\\nRequirement already satisfied, skipping upgrade: pandas>=0.20.2 in /usr/local/lib/python3.7/site-packages (from azureml-model-management-sdk==1.0.1b6.post1->azureml-defaults) (1.1.2)\\nCollecting dill>=0.2.7.1\\n  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\\nCollecting itsdangerous>=0.24\\r\\n  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\\nCollecting click>=5.1\\n  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\\nRequirement already satisfied, skipping upgrade: Jinja2>=2.10 in /usr/local/lib/python3.7/site-packages (from flask==1.0.3->azureml-defaults) (2.11.2)\\nRequirement already satisfied, skipping upgrade: importlib-metadata; python_version < \\\"3.8\\\" in /usr/local/lib/python3.7/site-packages (from jsonpickle->azureml-core) (1.7.0)\\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.1->azureml-core) (2020.12.5)\\nRequirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.1->azureml-core) (3.0.4)\\nRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.19.1->azureml-core) (2.10)\\nCollecting backports.weakref\\n  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\\nCollecting jeepney>=0.6\\n  Downloading jeepney-0.6.0-py3-none-any.whl (45 kB)\\nRequirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.7/site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core) (1.14.3)\\nCollecting pyasn1>=0.1.1\\n  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\\nCollecting websocket-client>=0.32.0\\n  Downloading websocket_client-0.57.0-py2.py3-none-any.whl (200 kB)\\nCollecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \\\"CPython\\\" and python_version < \\\"3.9\\\"\\n  Downloading ruamel.yaml.clib-0.2.2-cp37-cp37m-manylinux1_x86_64.whl (547 kB)\\nCollecting isodate>=0.6.0\\n  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\\nCollecting requests-oauthlib>=0.5.0\\n  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\\nCollecting azureml-dataprep-rslex<1.8.0a,>=1.7.0dev0\\n  Downloading azureml_dataprep_rslex-1.7.0-cp37-cp37m-manylinux2010_x86_64.whl (8.5 MB)\\nCollecting azure-identity<1.5.0,>=1.2.0\\n  Downloading azure_identity-1.4.1-py2.py3-none-any.whl (86 kB)\\nCollecting dotnetcore2<3.0.0,>=2.1.14\\n  Downloading dotnetcore2-2.1.20-py3-none-manylinux1_x86_64.whl (28.7 MB)\\nCollecting cloudpickle<2.0.0,>=1.1.0\\r\\n  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\\nCollecting azureml-dataprep-native<30.0.0,>=29.0.0\\n  Downloading azureml_dataprep_native-29.0.0-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\\nRequirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/site-packages (from Jinja2>=2.10->flask==1.0.3->azureml-defaults) (1.1.1)\\nRequirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata; python_version < \\\"3.8\\\"->jsonpickle->azureml-core) (3.1.0)\\nRequirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core) (2.20)\\nCollecting oauthlib>=3.0.0\\n  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\\nCollecting msal<2.0.0,>=1.3.0\\n  Downloading msal-1.9.0-py2.py3-none-any.whl (59 kB)\\nCollecting msal-extensions~=0.2.2\\n  Downloading msal_extensions-0.2.2-py2.py3-none-any.whl (15 kB)\\nCollecting azure-core<2.0.0,>=1.0.0\\n  Downloading azure_core-1.11.0-py2.py3-none-any.whl (127 kB)\\nCollecting distro>=1.2.0\\n  Downloading distro-1.5.0-py2.py3-none-any.whl (18 kB)\\nCollecting portalocker~=1.0; platform_system != \\\"Windows\\\"\\n  Downloading portalocker-1.7.1-py2.py3-none-any.whl (10 kB)\\nBuilding wheels for collected packages: json-logging-py, fusepy, liac-arff\\n  Building wheel for json-logging-py (setup.py): started\\n  Building wheel for json-logging-py (setup.py): finished with status 'done'\\n  Created wheel for json-logging-py: filename=json_logging_py-0.2-py3-none-any.whl size=3924 sha256=24bf04bccea2a7a4577f7138672c782bd670ea3aeea52dfa7c4b76d827708a7f\\n  Stored in directory: /tmp/pip-ephem-wheel-cache-qv8pr1e_/wheels/2b/2c/0b/56aba27cc60071c52f66346a1abc22ee9db8c7376549aa4910\\n  Building wheel for fusepy (setup.py): started\\n  Building wheel for fusepy (setup.py): finished with status 'done'\\n  Created wheel for fusepy: filename=fusepy-3.0.1-py3-none-any.whl size=10504 sha256=de4f2ef691dece8436d9874e884fe0cc95d82f85874b1474d645df601665e0f6\\n  Stored in directory: /tmp/pip-ephem-wheel-cache-qv8pr1e_/wheels/89/07/84/a5ebfafeefbbc56ceda9d6935a54a8be7a4eccf4ea7e9bf980\\n  Building wheel for liac-arff (setup.py): started\\n  Building wheel for liac-arff (setup.py): finished with status 'done'\\n  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11730 sha256=50d57b935bf007a2577a7002265ba6152d00918358bfb59eedae231c29c58130\\n  Stored in directory: /tmp/pip-ephem-wheel-cache-qv8pr1e_/wheels/1f/0f/15/332ca86cbebf25ddf98518caaf887945fbe1712b97a0f2493b\\nSuccessfully built json-logging-py fusepy liac-arff\\nInstalling collected packages: pyarrow, azureml-dataprep-rslex, PyJWT, msal, portalocker, msal-extensions, azure-core, azure-identity, distro, dotnetcore2, cloudpickle, azureml-dataprep-native, azureml-dataprep, fusepy, azureml-dataset-runtime, isodate, oauthlib, requests-oauthlib, msrest, azure-common, adal, msrestazure, azure-mgmt-storage, azure-graphrbac, jsonpickle, azure-mgmt-resource, jmespath, pathspec, backports.weakref, backports.tempfile, jeepney, SecretStorage, azure-mgmt-containerregistry, azure-mgmt-authorization, contextlib2, pyasn1, ndg-httpsclient, websocket-client, docker, ruamel.yaml.clib, ruamel.yaml, azure-mgmt-keyvault, azureml-core, configparser, applicationinsights, liac-arff, dill, azureml-model-management-sdk, itsdangerous, werkzeug, click, flask, gunicorn, json-logging-py, azureml-defaults, pip, install, absl-py, joblib, threadpoolctl, scipy, scikit-learn\\r\\n\\nError occurred: Image build failed. For more details, check log file azureml-logs/20_image_build_log.txt.\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.22.0\"}, \"loading\": false}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 34,
      "metadata": {
        "gather": {
          "logged": 1613718591110
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the run is succesfully finished, you can check the metrics logged."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# get all metris logged in the run\n",
        "metrics = run.get_metrics()\n",
        "print(metrics)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Regularization Rate': 0.01, 'Accuracy': 0.9069767441860465}\n"
          ]
        }
      ],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1613519593518
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# register the generated model\n",
        "model = run.register_model(model_name='iris.model', model_path='outputs/iris.model')"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1613519596779
        }
      }
    }
  ],
  "metadata": {
    "index_order": 1,
    "exclude_from_index": false,
    "task": "Submiting a run on a spark cluster",
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "sanpil"
      }
    ],
    "kernel_info": {
      "name": "python3-azureml"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "HDI cluster"
    ],
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "tags": [
      "None"
    ],
    "datasets": [
      "None"
    ],
    "categories": [
      "how-to-use-azureml",
      "training"
    ],
    "category": "training",
    "framework": [
      "PySpark"
    ],
    "friendly_name": "Training in Spark",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}