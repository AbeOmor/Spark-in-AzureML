{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/training/train-in-spark/train-in-spark.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train in Spark on AML Compute\n",
        "* Create Workspace\n",
        "*\tCreate Experiment\n",
        "*\tCopy relevant files to the script folder\n",
        "*\tCreate Environment from AMLSpark Curated Environment\n",
        "*\tCreate Datastore and DataRef to mount data onto Spark Cluster\n",
        "*\tCreate an AML Run Config with the PySpark Framework\n",
        "*\tConfigure and Run Script on AML Compute configured for Spark\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't already to establish your connection to the AzureML Workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check core SDK version number\n",
        "import azureml.core\n",
        "\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDK version: 1.23.0\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1614050585123
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Workspace\n",
        "\n",
        "Initialize a workspace object from persisted configuration."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zhenzhuuksouth\n",
            "zhenzhuuksouth\n",
            "uksouth\n",
            "e9b2ec51-5c94-4fa8-809a-dc1e695e4896\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1614050585816
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Experiment\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = 'data-processing-on-spark'\n",
        "\n",
        "from azureml.core import Experiment\n",
        "exp = Experiment(workspace=ws, name=experiment_name)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1614050586168
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View `train-spark.py`\n",
        "\n",
        "For convenience, we created a training script for you. It is printed below as a text, but you can also run `%pfile ./train-spark.py` in a cell to show the file."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "with open('stackoverflow-data-prep.py', 'r') as training_script:\n",
        "    print(training_script.read())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#!/usr/bin/env python\n",
            "# coding: utf-8\n",
            "\n",
            "# Copyright (c) Microsoft. All rights reserved.\n",
            "# Licensed under the MIT license.\n",
            "\n",
            "import numpy as np\n",
            "import pyspark\n",
            "import os\n",
            "import urllib\n",
            "import sys\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "from sklearn.utils import shuffle\n",
            "import os\n",
            "\n",
            "from pyspark.sql.functions import *\n",
            "from pyspark.ml.classification import *\n",
            "from pyspark.ml.evaluation import *\n",
            "from pyspark.ml.feature import *\n",
            "from pyspark.sql.types import StructType, StructField\n",
            "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
            "\n",
            "from azureml.core.run import Run\n",
            "\n",
            "# initialize logger\n",
            "run = Run.get_context()\n",
            "\n",
            "# start Spark session\n",
            "\n",
            "spark = pyspark.sql.SparkSession.builder.appName('Stackoverflow') \\\n",
            "             .getOrCreate()\n",
            "           # .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.11:0.6.0\") \\\n",
            "           # .config(\"spark.jars.repositories\", \"https://mvnrepository.com/artifact/com.databricks/spark-xml\") \\\n",
            "            \n",
            "\n",
            "\n",
            "# print runtime versions\n",
            "print('****************')\n",
            "print('Python version: {}'.format(sys.version))\n",
            "print('Spark version: {}'.format(spark.version))\n",
            "print('****************')\n",
            "'''\n",
            "STEP 1: Download Stack Overflow data from archive. (This takes about 2-3 hours)\n",
            "'''\n",
            "# get_ipython().system('wget https://archive.org/download/stackexchange/stackoverflow.com-Posts.7z')\n",
            "# get_ipython().system('sudo apt-get install p7zip-full')\n",
            "# get_ipython().system('7z x stackoverflow.com-Posts.7z -oposts')\n",
            "\n",
            "# Define input arguments\n",
            "import sys\n",
            "data_dir = sys.argv[1]\n",
            "#data_dir = '../../../../../stackoverflow'\n",
            "xml_file_path = str(os.path.join(data_dir, 'stackoverflow/posts/Posts.xml'))\n",
            "#xml_file_path = 'wasbs://zhenzhuuksouth3632161177.blob.core.windows.net/stackoverflowdata/stackoverflow/posts/Posts.xml'\n",
            "print('*******TESTING PATHS FOR DATA*********')\n",
            "print(xml_file_path)\n",
            "print(os.listdir(str(os.path.join(data_dir, 'stackoverflow'))))\n",
            "print('****************')\n",
            "\n",
            "'''\n",
            "STEP 2: Copy data over to databricks file system (This takes about an hour)\n",
            "'''\n",
            "## TODO Need to figure out how to move data to to a mounted Hadoop Storage for the AML Cluster\n",
            "#dbutils.fs.cp(xml_file_path, \"dbfs:/tmp/posts/Posts.xml\")  \n",
            "\n",
            "'''\n",
            "STEP 3: Process data using Spark\n",
            "\n",
            "Note - This requires the spark-xml maven library (com.databricks:spark-xml_2.11:0.6.0) to be installed.\n",
            "'''\n",
            "import pyspark\n",
            "from pyspark.sql import functions as sf\n",
            "from pyspark.sql import Row\n",
            "from pyspark.sql.functions import size, col, concat_ws, rtrim, regexp_replace, split, udf\n",
            "from pyspark.sql.types import ArrayType\n",
            "\n",
            "# load xml file into spark data frame.\n",
            "posts = spark.read.format(\"xml\").option(\"rowTag\", \"row\").load(xml_file_path)\n",
            "\n",
            "# select only questions\n",
            "questions = posts.filter(posts._PostTypeId == 1) \n",
            "\n",
            "# drop irrelvant columns and clean up strings\n",
            "questions = questions.select([c for c in questions.columns if c in ['_Id','_Title','_Body','_Tags']])\n",
            "questions = questions.withColumn('full_question', sf.concat(sf.col('_Title'), sf.lit(' '), sf.col('_Body')))\n",
            "questions = questions.select([c for c in questions.columns if c in ['_Id','full_question','_Tags']]).withColumn(\"full_question\", regexp_replace(\"full_question\", \"[\\\\n,]\", \" \"))\n",
            "questions = questions.withColumn(\"_Tags\", regexp_replace(\"_Tags\", \"><\", \" \"))\n",
            "questions = questions.withColumn(\"_Tags\", regexp_replace(\"_Tags\", \"(>|<)\", \"\"))\n",
            "questions = questions.withColumn('_Tags', rtrim(questions._Tags))\n",
            "questions = questions.withColumn('_Tags', split(questions._Tags, \" \"))\n",
            "\n",
            "# filter out to single tags in following list\n",
            "tags_of_interest = ['azure-devops', 'azure-functions', 'azure-web-app-service', 'azure-storage', 'azure-virtual-machine'] \n",
            "\n",
            "def intersect(xs):\n",
            "    xs = set(xs)\n",
            "    @udf(\"array<string>\")\n",
            "    def _(ys):\n",
            "        return list(xs.intersection(ys))\n",
            "    return _\n",
            "\n",
            "questions = questions.withColumn(\"intersect\", intersect(tags_of_interest)(\"_Tags\"))\n",
            "questions = questions.filter(size(col(\"intersect\"))==1)\n",
            "questions = questions.select('_Id', 'full_question', 'intersect').withColumn('_Tags', concat_ws(', ', 'intersect'))\n",
            "questions = questions.select('_Id', 'full_question', '_Tags')\n",
            "\n",
            "questions.show()\n",
            "\n",
            "\n",
            "'''\n",
            "Step 4: Convert processed data into pandas data frame for final preprocessing and data split\n",
            "'''\n",
            "\n",
            "df = questions.toPandas()\n",
            "\n",
            "# drop nan values and remove line breaks\n",
            "df.dropna(inplace=True)\n",
            "df.replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
            "\n",
            "# balance dataset \n",
            "balanced = df.groupby('_Tags')\n",
            "balanced.apply(lambda x: x.sample(balanced.size().min())).reset_index(drop=True).to_csv('balanced.csv')\n",
            "bd = pd.read_csv('balanced.csv')\n",
            "bd.drop('Unnamed: 0', axis=1, inplace=True)\n",
            "\n",
            "# shuffle data \n",
            "# bd = shuffle(bd)\n",
            "\n",
            "# split data into train, test, and valid sets\n",
            "msk = np.random.rand(len(bd)) < 0.7\n",
            "train = bd[msk]\n",
            "temp = bd[~msk]\n",
            "msk = np.random.rand(len(temp)) < 0.66\n",
            "valid = temp[msk]\n",
            "test = temp[~msk]\n",
            "\n",
            "'''\n",
            "STEP 5: Save dataset into csv and class.txt files\n",
            "'''\n",
            "\n",
            "output_dir = str(os.path.join(data_dir, 'stackoverflow/output'))\n",
            "\n",
            "\n",
            "if not os.path.exists(output_dir):\n",
            "    os.makedirs(output_dir)\n",
            "\n",
            "# create and save classes.txt file\n",
            "classes = pd.DataFrame(bd['_Tags'].unique().tolist())\n",
            "classes.to_csv(os.path.join(output_dir, 'classes.txt'), header=False, index=False)\n",
            "\n",
            "# save train, valid, and test files\n",
            "train.to_csv(os.path.join(output_dir, 'train.csv'), header=False, index=False)\n",
            "valid.to_csv(os.path.join(output_dir, 'valid.csv'), header=False, index=False)\n",
            "test.to_csv(os.path.join(output_dir, 'test.csv'), header=False, index=False)\n",
            "\n"
          ]
        }
      ],
      "execution_count": 34,
      "metadata": {
        "gather": {
          "logged": 1614112175324
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure & Run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** You can use Docker-based execution to run the Spark job in local computer or a remote VM. Please see the `train-in-remote-vm` notebook for example on how to configure and run in Docker mode in a VM. Make sure you choose a Docker image that has Spark installed, such as `microsoft/mmlspark:0.12`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attach an AML Compute\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\r\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "# Choose a name for your CPU cluster\r\n",
        "cpu_cluster_name = \"spark-data-proc\"\r\n",
        "\r\n",
        "# Verify that the cluster does not exist already\r\n",
        "try:\r\n",
        "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D12_V2',\r\n",
        "                                                           max_nodes=4, \r\n",
        "                                                           vm_priority=\"lowpriority\",\r\n",
        "                                                           idle_seconds_before_scaledown=2400)\r\n",
        "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\r\n",
        "\r\n",
        "cpu_cluster.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing cluster, use it.\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1614050587312
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Environment "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure AML Curated Environemnt and custom JAR packages with a pip environment that has `scikit-learn`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.environment import Environment\n",
        "from azureml.core import RunConfiguration\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "spark_env=Environment.get(workspace=ws, name=\"AzureML-PySpark-MmlSpark-0.15\")\n",
        "spark_env = spark_env.clone(\"PySpark-MmlSpark-Alt\")\n",
        "\n",
        "# Add \n",
        "conda_dep = CondaDependencies()\n",
        "\n",
        "# Installs scikit-learn pip package\n",
        "conda_dep.add_pip_package(\"scikit-learn\")\n",
        "\n",
        "# Adds dependencies to PythonSection of myenv\n",
        "spark_env.python.conda_dependencies=conda_dep\n",
        "spark_env.spark.packages = [{\"group\": \"com.databricks\",\"artifact\": \"spark-xml_2.11\",\"version\": \"0.6.0\"}]\n",
        "spark_env.spark.repositories = [\"https://mvnrepository.com/artifact/com.databricks/spark-xml\"]\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1614050587737
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure AML Cluster "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the Framework as PySpark, set cluster name and environment the AML Cluster will use"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use pyspark framework\r\n",
        "spark_run_config = RunConfiguration(framework=\"PySpark\")\r\n",
        "\r\n",
        "# Set compute target to the cpu cluster\r\n",
        "spark_run_config.target = cpu_cluster.name\r\n",
        "\r\n",
        "# Set environment\r\n",
        "spark_run_config.environment = spark_env"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Datastore and DataReference for Spark to talk to"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO-DO, write instruction on how to mount to a Blob datastore"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Datastore\r\n",
        "\r\n",
        "stackoverflow_datastore = ws.datastores['stackoverflow_blob']\r\n",
        "data_ref = stackoverflow_datastore.as_mount()"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1614111059473
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submit the script to AzureML Compute"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import ScriptRunConfig, Environment\n",
        "\n",
        "script_run_config = ScriptRunConfig(source_directory = '.',\n",
        "                                    script= 'stackoverflow-data-prep.py',\n",
        "                                    arguments=[str(data_ref)],\n",
        "                                    run_config = spark_run_config,\n",
        "                                    )\n",
        "script_run_config.run_config.data_references[data_ref.data_reference_name] = data_ref.to_config()\n",
        "run = exp.submit(config=script_run_config)\n"
      ],
      "outputs": [],
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1614111064694
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monitor the run using a Juypter widget"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "RunDetails(run).show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b45c14778ef4aa98120e1a898746796"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/data-processing-on-spark/runs/data-processing-on-spark_1614050824_f3151816?wsid=/subscriptions/e9b2ec51-5c94-4fa8-809a-dc1e695e4896/resourcegroups/zhenzhuuksouth/workspaces/zhenzhuuksouth\", \"run_id\": \"data-processing-on-spark_1614050824_f3151816\", \"run_properties\": {\"run_id\": \"data-processing-on-spark_1614050824_f3151816\", \"created_utc\": \"2021-02-23T03:27:09.118979Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"d88f4816-edf9-4c51-989f-bfe6f6a621a2\", \"azureml.git.repository_uri\": \"git@github.com:AbeOmor/Spark-in-AzureML.git\", \"mlflow.source.git.repoURL\": \"git@github.com:AbeOmor/Spark-in-AzureML.git\", \"azureml.git.branch\": \"main\", \"mlflow.source.git.branch\": \"main\", \"azureml.git.commit\": \"3fc855f2bf6616973c0122185bd6ba61e19247aa\", \"mlflow.source.git.commit\": \"3fc855f2bf6616973c0122185bd6ba61e19247aa\", \"azureml.git.dirty\": \"True\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {\"_aml_system_ComputeTargetStatus\": \"{\\\"AllocationState\\\":\\\"steady\\\",\\\"PreparingNodeCount\\\":0,\\\"RunningNodeCount\\\":0,\\\"CurrentNodeCount\\\":0}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"azureml-logs/20_image_build_log.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614050824_f3151816/azureml-logs/20_image_build_log.txt?sv=2019-02-02&sr=b&sig=634jtgMiUOPm5eapQYAFLN8v1dcGWLBTnNYNi6wjPus%3D&st=2021-02-23T03%3A27%3A20Z&se=2021-02-23T11%3A37%3A20Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_62b30fe30a9a51f2fcd0b5597a59b54a3b36bebf055c0a07fd7169d02be80f08_p.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614050824_f3151816/azureml-logs/55_azureml-execution-tvmps_62b30fe30a9a51f2fcd0b5597a59b54a3b36bebf055c0a07fd7169d02be80f08_p.txt?sv=2019-02-02&sr=b&sig=rlNjrEWbzRwp6VSNBy%2B80GR0PTTjTFVi2NOlJQ2JBJA%3D&st=2021-02-23T03%3A27%3A20Z&se=2021-02-23T11%3A37%3A20Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_62b30fe30a9a51f2fcd0b5597a59b54a3b36bebf055c0a07fd7169d02be80f08_p.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614050824_f3151816/azureml-logs/65_job_prep-tvmps_62b30fe30a9a51f2fcd0b5597a59b54a3b36bebf055c0a07fd7169d02be80f08_p.txt?sv=2019-02-02&sr=b&sig=9Tc%2BCI%2BijeWjnawvsJii%2BVhXggUotgwatptGN%2F5Fe0A%3D&st=2021-02-23T03%3A27%3A20Z&se=2021-02-23T11%3A37%3A20Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614050824_f3151816/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=X1vesHRYo6rQk6aTD3vu3sLQ%2FU2LC9dqYC%2Fn%2BaQvtZk%3D&st=2021-02-23T03%3A27%3A20Z&se=2021-02-23T11%3A37%3A20Z&sp=r\", \"azureml-logs/process_info.json\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614050824_f3151816/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=8WhOz%2BtJRS%2Bd9rJyS59rGdGuBi008VB5EpM%2FyUBCbdA%3D&st=2021-02-23T03%3A27%3A20Z&se=2021-02-23T11%3A37%3A20Z&sp=r\", \"azureml-logs/process_status.json\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614050824_f3151816/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=V472METviATJLYddjOlfrVeHiU8gRAQaIzVEQFdbzRY%3D&st=2021-02-23T03%3A27%3A20Z&se=2021-02-23T11%3A37%3A20Z&sp=r\", \"logs/azureml/539_azureml.log\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614050824_f3151816/logs/azureml/539_azureml.log?sv=2019-02-02&sr=b&sig=ubgn6h8bqAEK7o0buxRmivPIGFT8oz2G1zOtwJ1Vy5U%3D&st=2021-02-23T03%3A27%3A20Z&se=2021-02-23T11%3A37%3A20Z&sp=r\", \"logs/azureml/job_prep_azureml.log\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614050824_f3151816/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=qP10nbeQLIdrNT%2BzK%2BotFvkHpA19afPbIWxD6kg2l60%3D&st=2021-02-23T03%3A27%3A20Z&se=2021-02-23T11%3A37%3A20Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\", \"logs/azureml/job_prep_azureml.log\"], [\"azureml-logs/20_image_build_log.txt\"], [\"azureml-logs/55_azureml-execution-tvmps_62b30fe30a9a51f2fcd0b5597a59b54a3b36bebf055c0a07fd7169d02be80f08_p.txt\"], [\"azureml-logs/65_job_prep-tvmps_62b30fe30a9a51f2fcd0b5597a59b54a3b36bebf055c0a07fd7169d02be80f08_p.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"logs/azureml/539_azureml.log\"]], \"run_duration\": \"0:16:26\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"2021-02-23 03:36:27,771|azureml|DEBUG|Inputs:: kwargs: {'OutputCollection': True, 'EnableMLflowTracking': True, 'snapshotProject': True, 'only_in_process_features': True, 'skip_track_logs_dir': True}, track_folders: None, deny_list: None, directories_to_watch: []\\n2021-02-23 03:36:27,772|azureml.history._tracking.PythonWorkingDirectory|DEBUG|Execution target type: batchai\\n2021-02-23 03:36:28,009|azureml.history._tracking.PythonWorkingDirectory|DEBUG|PySpark found in environment.\\n2021-02-23 03:36:28,009|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Pinning working directory for filesystems: ['pyfs']\\n2021-02-23 03:36:28,411|azureml.core.run|DEBUG|Adding new factory <function ScriptRun._from_run_dto at 0x7efd6f056488> for run source azureml.scriptrun\\n2021-02-23 03:36:28,412|azureml.core.run|DEBUG|Identity in use is not set. Falling back to using AMLToken\\n2021-02-23 03:36:28,412|azureml.core.run|DEBUG|Using AMLToken auth for remote run\\n2021-02-23 03:36:28,414|azureml.core.authentication.TokenRefresherDaemon|DEBUG|Starting daemon and triggering first instance\\n2021-02-23 03:36:28,422|azureml._restclient.clientbase|INFO|Created a worker pool for first use\\n2021-02-23 03:36:28,422|azureml.core.authentication|DEBUG|Time to expire 1813840.577164 seconds\\n2021-02-23 03:36:28,423|azureml._restclient.service_context|DEBUG|Created a static thread pool for ServiceContext class\\n2021-02-23 03:36:28,423|azureml._restclient.clientbase|DEBUG|ClientBase: Calling get with url None\\n2021-02-23 03:36:28,456|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://uksouth.experiments.azureml.net.\\n2021-02-23 03:36:28,457|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://uksouth.experiments.azureml.net.\\n2021-02-23 03:36:28,457|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://uksouth.experiments.azureml.net.\\n2021-02-23 03:36:28,457|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://uksouth.experiments.azureml.net.\\n2021-02-23 03:36:28,457|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://uksouth.experiments.azureml.net.\\n2021-02-23 03:36:28,458|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://uksouth.experiments.azureml.net.\\n2021-02-23 03:36:28,458|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://uksouth.experiments.azureml.net.\\n2021-02-23 03:36:28,499|azureml._SubmittedRun#data-processing-on-spark_1614050824_f3151816.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[START]\\n2021-02-23 03:36:28,499|azureml._SubmittedRun#data-processing-on-spark_1614050824_f3151816.RunHistoryFacade.RunClient|DEBUG|ClientBase: Calling get_by_exp_id with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experimentids/{experimentId}/runs/{runId}\\n2021-02-23 03:36:28,573|azureml._SubmittedRun#data-processing-on-spark_1614050824_f3151816.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[STOP]\\n2021-02-23 03:36:28,574|azureml._SubmittedRun#data-processing-on-spark_1614050824_f3151816|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'amlcompute', 'ContentSnapshotId': 'd88f4816-edf9-4c51-989f-bfe6f6a621a2', 'azureml.git.repository_uri': 'git@github.com:AbeOmor/Spark-in-AzureML.git', 'mlflow.source.git.repoURL': 'git@github.com:AbeOmor/Spark-in-AzureML.git', 'azureml.git.branch': 'main', 'mlflow.source.git.branch': 'main', 'azureml.git.commit': '3fc855f2bf6616973c0122185bd6ba61e19247aa', 'mlflow.source.git.commit': '3fc855f2bf6616973c0122185bd6ba61e19247aa', 'azureml.git.dirty': 'True', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}\\n2021-02-23 03:36:28,574|azureml._SubmittedRun#data-processing-on-spark_1614050824_f3151816.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2021-02-23 03:36:28,575|azureml|WARNING|Could not import azureml.mlflow or azureml.contrib.mlflow mlflow APIs will not run against AzureML services.  Add azureml-mlflow as a conda dependency for the run if this behavior is desired\\n2021-02-23 03:36:28,575|azureml.WorkerPool|DEBUG|[START]\\n2021-02-23 03:36:28,575|azureml.SendRunKillSignal|DEBUG|[START]\\n2021-02-23 03:36:28,575|azureml.RunStatusContext|DEBUG|[START]\\n2021-02-23 03:36:28,575|azureml._SubmittedRun#data-processing-on-spark_1614050824_f3151816.RunContextManager.RunStatusContext|DEBUG|[START]\\n2021-02-23 03:36:28,575|azureml.MetricsClient|DEBUG|[START]\\n2021-02-23 03:36:28,575|azureml._SubmittedRun#data-processing-on-spark_1614050824_f3151816.RunHistoryFacade.MetricsClient|DEBUG|[START]\\n2021-02-23 03:36:28,575|azureml.WorkingDirectoryCM|DEBUG|[START]\\n2021-02-23 03:36:28,576|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|[START]\\n2021-02-23 03:36:28,576|azureml.history._tracking.PythonWorkingDirectory|INFO|Current working dir: /mnt/batch/tasks/shared/LS_root/jobs/zhenzhuuksouth/azureml/data-processing-on-spark_1614050824_f3151816/mounts/workspaceblobstore/azureml/data-processing-on-spark_1614050824_f3151816\\n2021-02-23 03:36:28,576|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Calling pyfs\\n2021-02-23 03:36:28,576|azureml.history._tracking.PythonWorkingDirectory.workingdir|DEBUG|Storing working dir for pyfs as /mnt/batch/tasks/shared/LS_root/jobs/zhenzhuuksouth/azureml/data-processing-on-spark_1614050824_f3151816/mounts/workspaceblobstore/azureml/data-processing-on-spark_1614050824_f3151816\\n2021-02-23 03:36:30,512|azureml.core.run|DEBUG|Identity in use is not set. Falling back to using AMLToken\\n2021-02-23 03:36:30,512|azureml.core.run|DEBUG|Using AMLToken auth for remote run\\n2021-02-23 03:36:30,512|azureml._restclient.service_context|DEBUG|Access an existing static threadpool for ServiceContext class\\n2021-02-23 03:36:30,512|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://uksouth.experiments.azureml.net.\\n2021-02-23 03:36:30,513|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://uksouth.experiments.azureml.net.\\n2021-02-23 03:36:30,513|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://uksouth.experiments.azureml.net.\\n2021-02-23 03:36:30,513|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://uksouth.experiments.azureml.net.\\n2021-02-23 03:36:30,513|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://uksouth.experiments.azureml.net.\\n2021-02-23 03:36:30,513|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://uksouth.experiments.azureml.net.\\n2021-02-23 03:36:30,514|azureml._base_sdk_common.service_discovery|DEBUG|Found history service url in environment variable AZUREML_SERVICE_ENDPOINT, history service url: https://uksouth.experiments.azureml.net.\\n2021-02-23 03:36:30,544|azureml._SubmittedRun#data-processing-on-spark_1614050824_f3151816.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[START]\\n2021-02-23 03:36:30,545|azureml._SubmittedRun#data-processing-on-spark_1614050824_f3151816.RunHistoryFacade.RunClient|DEBUG|ClientBase: Calling get_by_exp_id with url /history/v1.0/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.MachineLearningServices/workspaces/{workspaceName}/experimentids/{experimentId}/runs/{runId}\\n2021-02-23 03:36:30,604|azureml._SubmittedRun#data-processing-on-spark_1614050824_f3151816.RunHistoryFacade.RunClient.get_by_exp_id-async:False|DEBUG|[STOP]\\n2021-02-23 03:36:30,605|azureml._SubmittedRun#data-processing-on-spark_1614050824_f3151816|DEBUG|Constructing run from dto. type: azureml.scriptrun, source: None, props: {'_azureml.ComputeTargetType': 'amlcompute', 'ContentSnapshotId': 'd88f4816-edf9-4c51-989f-bfe6f6a621a2', 'azureml.git.repository_uri': 'git@github.com:AbeOmor/Spark-in-AzureML.git', 'mlflow.source.git.repoURL': 'git@github.com:AbeOmor/Spark-in-AzureML.git', 'azureml.git.branch': 'main', 'mlflow.source.git.branch': 'main', 'azureml.git.commit': '3fc855f2bf6616973c0122185bd6ba61e19247aa', 'mlflow.source.git.commit': '3fc855f2bf6616973c0122185bd6ba61e19247aa', 'azureml.git.dirty': 'True', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}\\n2021-02-23 03:36:30,605|azureml._SubmittedRun#data-processing-on-spark_1614050824_f3151816.RunContextManager|DEBUG|Valid logs dir, setting up content loader\\n2021-02-23 03:36:58,415|azureml.core.authentication|DEBUG|Time to expire 1813810.584898 seconds\\n2021-02-23 03:37:28,415|azureml.core.authentication|DEBUG|Time to expire 1813780.584594 seconds\\n2021-02-23 03:37:58,415|azureml.core.authentication|DEBUG|Time to expire 1813750.58432 seconds\\n2021-02-23 03:38:28,416|azureml.core.authentication|DEBUG|Time to expire 1813720.584056 seconds\\n2021-02-23 03:38:58,416|azureml.core.authentication|DEBUG|Time to expire 1813690.583812 seconds\\n2021-02-23 03:39:28,416|azureml.core.authentication|DEBUG|Time to expire 1813660.583387 seconds\\n2021-02-23 03:39:58,417|azureml.core.authentication|DEBUG|Time to expire 1813630.583023 seconds\\n2021-02-23 03:40:28,417|azureml.core.authentication|DEBUG|Time to expire 1813600.582746 seconds\\n2021-02-23 03:40:58,417|azureml.core.authentication|DEBUG|Time to expire 1813570.582495 seconds\\n2021-02-23 03:41:28,417|azureml.core.authentication|DEBUG|Time to expire 1813540.582206 seconds\\n2021-02-23 03:41:58,418|azureml.core.authentication|DEBUG|Time to expire 1813510.581822 seconds\\n2021-02-23 03:42:28,418|azureml.core.authentication|DEBUG|Time to expire 1813480.581529 seconds\\n2021-02-23 03:42:58,418|azureml.core.authentication|DEBUG|Time to expire 1813450.581168 seconds\\n2021-02-23 03:43:28,419|azureml.core.authentication|DEBUG|Time to expire 1813420.580832 seconds\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.23.0\"}, \"loading\": false}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1614051680366
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the run is succesfully finished, you can check the metrics logged."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "index_order": 1,
    "exclude_from_index": false,
    "task": "Submiting a run on a spark cluster",
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "sanpil"
      }
    ],
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "HDI cluster"
    ],
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "tags": [
      "None"
    ],
    "datasets": [
      "None"
    ],
    "categories": [
      "how-to-use-azureml",
      "training"
    ],
    "category": "training",
    "framework": [
      "PySpark"
    ],
    "friendly_name": "Training in Spark",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}