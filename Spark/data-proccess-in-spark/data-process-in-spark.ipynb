{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) Microsoft Corporation. All rights reserved.\n",
        "\n",
        "Licensed under the MIT License."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/how-to-use-azureml/training/train-in-spark/train-in-spark.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train in Spark on AML Compute\n",
        "* Create Workspace\n",
        "*\tCreate Experiment\n",
        "*\tCopy relevant files to the script folder\n",
        "*\tCreate Environment from AMLSpark Curated Environment\n",
        "*\tCreate Datastore and DataRef to mount data onto Spark Cluster\n",
        "*\tCreate an AML Run Config with the PySpark Framework\n",
        "*\tConfigure and Run Script on AML Compute configured for Spark\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration](../../../configuration.ipynb) Notebook first if you haven't already to establish your connection to the AzureML Workspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check core SDK version number\n",
        "import azureml.core\n",
        "\n",
        "print(\"SDK version:\", azureml.core.VERSION)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SDK version: 1.22.0\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1614123245577
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Workspace\n",
        "\n",
        "Initialize a workspace object from persisted configuration."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "ws = Workspace.from_config()\n",
        "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zhenzhuuksouth\n",
            "zhenzhuuksouth\n",
            "uksouth\n",
            "e9b2ec51-5c94-4fa8-809a-dc1e695e4896\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1614123250551
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Experiment\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_name = 'data-processing-on-spark'\n",
        "\n",
        "from azureml.core import Experiment\n",
        "exp = Experiment(workspace=ws, name=experiment_name)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1614123250999
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View `train-spark.py`\n",
        "\n",
        "For convenience, we created a training script for you. It is printed below as a text, but you can also run `%pfile ./train-spark.py` in a cell to show the file."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "with open('stackoverflow-data-prep.py', 'r') as training_script:\n",
        "    print(training_script.read())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#!/usr/bin/env python\n",
            "# coding: utf-8\n",
            "\n",
            "# Copyright (c) Microsoft. All rights reserved.\n",
            "# Licensed under the MIT license.\n",
            "\n",
            "import numpy as np\n",
            "import pyspark\n",
            "import os\n",
            "import urllib\n",
            "import sys\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "from sklearn.utils import shuffle\n",
            "import os\n",
            "\n",
            "from pyspark.sql.functions import *\n",
            "from pyspark.ml.classification import *\n",
            "from pyspark.ml.evaluation import *\n",
            "from pyspark.ml.feature import *\n",
            "from pyspark.sql.types import StructType, StructField\n",
            "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
            "\n",
            "from azureml.core.run import Run\n",
            "\n",
            "# initialize logger\n",
            "run = Run.get_context()\n",
            "\n",
            "# start Spark session\n",
            "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
            "print(\"Spark session created\")\n",
            "\n",
            "# print runtime versions\n",
            "'''\n",
            "print('****************')\n",
            "print('Python version: {}'.format(sys.version))\n",
            "print('Spark version: {}'.format(spark.version))\n",
            "print('****************')\n",
            "\n",
            "STEP 1: Download Stack Overflow data from archive. (This takes about 2-3 hours)\n",
            "'''\n",
            "\n",
            "# Define input arguments\n",
            "import sys\n",
            "data_dir = sys.argv[1]\n",
            "\n",
            "xml_file_path = str(os.path.join(data_dir, 'stackoverflow/posts/Posts.xml'))\n",
            "\n",
            "print('*******TESTING PATHS FOR DATA*********')\n",
            "print(xml_file_path)\n",
            "print(os.listdir(str(os.path.join(data_dir, 'stackoverflow'))))\n",
            "print('****************')\n",
            "\n",
            "\n",
            "'''\n",
            "STEP 2: Process data using Spark\n",
            "\n",
            "Note - This requires the spark-xml maven library (com.databricks:spark-xml_2.11:0.6.0) to be installed.\n",
            "'''\n",
            "import pyspark\n",
            "from pyspark.sql import functions as sf\n",
            "from pyspark.sql import Row\n",
            "from pyspark.sql.functions import size, col, concat_ws, rtrim, regexp_replace, split, udf\n",
            "from pyspark.sql.types import ArrayType\n",
            "\n",
            "# load xml file into spark data frame.\n",
            "posts = spark.read.format(\"xml\").option(\"rowTag\", \"row\").load(xml_file_path)\n",
            "\n",
            "# select only questions\n",
            "questions = posts.filter(posts._PostTypeId == 1) \n",
            "\n",
            "# drop irrelvant columns and clean up strings\n",
            "questions = questions.select([c for c in questions.columns if c in ['_Id','_Title','_Body','_Tags']])\n",
            "questions = questions.withColumn('full_question', sf.concat(sf.col('_Title'), sf.lit(' '), sf.col('_Body')))\n",
            "questions = questions.select([c for c in questions.columns if c in ['_Id','full_question','_Tags']]).withColumn(\"full_question\", regexp_replace(\"full_question\", \"[\\\\n,]\", \" \"))\n",
            "questions = questions.withColumn(\"_Tags\", regexp_replace(\"_Tags\", \"><\", \" \"))\n",
            "questions = questions.withColumn(\"_Tags\", regexp_replace(\"_Tags\", \"(>|<)\", \"\"))\n",
            "questions = questions.withColumn('_Tags', rtrim(questions._Tags))\n",
            "questions = questions.withColumn('_Tags', split(questions._Tags, \" \"))\n",
            "\n",
            "# filter out to single tags in following list\n",
            "tags_of_interest = ['azure-devops', 'azure-functions', 'azure-web-app-service', 'azure-storage', 'azure-virtual-machine'] \n",
            "\n",
            "def intersect(xs):\n",
            "    xs = set(xs)\n",
            "    @udf(\"array<string>\")\n",
            "    def _(ys):\n",
            "        return list(xs.intersection(ys))\n",
            "    return _\n",
            "\n",
            "questions = questions.withColumn(\"intersect\", intersect(tags_of_interest)(\"_Tags\"))\n",
            "questions = questions.filter(size(col(\"intersect\"))==1)\n",
            "questions = questions.select('_Id', 'full_question', 'intersect').withColumn('_Tags', concat_ws(', ', 'intersect'))\n",
            "questions = questions.select('_Id', 'full_question', '_Tags')\n",
            "\n",
            "questions.show()\n",
            "\n",
            "\n",
            "'''\n",
            "Step 3: Convert processed data into pandas data frame for final preprocessing and data split\n",
            "'''\n",
            "\n",
            "df = questions.toPandas()\n",
            "\n",
            "# drop nan values and remove line breaks\n",
            "df.dropna(inplace=True)\n",
            "df.replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)\n",
            "\n",
            "# balance dataset \n",
            "balanced = df.groupby('_Tags')\n",
            "balanced.apply(lambda x: x.sample(balanced.size().min())).reset_index(drop=True).to_csv('balanced.csv')\n",
            "bd = pd.read_csv('balanced.csv')\n",
            "bd.drop('Unnamed: 0', axis=1, inplace=True)\n",
            "\n",
            "# shuffle data \n",
            "# bd = shuffle(bd)\n",
            "\n",
            "# split data into train, test, and valid sets\n",
            "msk = np.random.rand(len(bd)) < 0.7\n",
            "train = bd[msk]\n",
            "temp = bd[~msk]\n",
            "msk = np.random.rand(len(temp)) < 0.66\n",
            "valid = temp[msk]\n",
            "test = temp[~msk]\n",
            "\n",
            "'''\n",
            "STEP 4: Save dataset into csv and class.txt files\n",
            "'''\n",
            "\n",
            "output_dir = str(os.path.join(data_dir, 'stackoverflow/output2'))\n",
            "\n",
            "\n",
            "if not os.path.exists(output_dir):\n",
            "    os.makedirs(output_dir)\n",
            "\n",
            "# create and save classes.txt file\n",
            "classes = pd.DataFrame(bd['_Tags'].unique().tolist())\n",
            "classes.to_csv(os.path.join(output_dir, 'classes.txt'), header=False, index=False)\n",
            "\n",
            "# save train, valid, and test files\n",
            "train.to_csv(os.path.join(output_dir, 'train.csv'), header=False, index=False)\n",
            "valid.to_csv(os.path.join(output_dir, 'valid.csv'), header=False, index=False)\n",
            "test.to_csv(os.path.join(output_dir, 'test.csv'), header=False, index=False)\n",
            "\n"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1614133408680
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure & Run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** You can use Docker-based execution to run the Spark job in local computer or a remote VM. Make sure you choose a Docker image that has Spark installed, such as `microsoft/mmlspark:0.15`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attach an AML Compute\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\r\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "\r\n",
        "# Choose a name for your CPU cluster\r\n",
        "cpu_cluster_name = \"spark-data-proc\"\r\n",
        "\r\n",
        "# Verify that the cluster does not exist already\r\n",
        "try:\r\n",
        "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\r\n",
        "    print('Found existing cluster, use it.')\r\n",
        "except ComputeTargetException:\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D12_V2',\r\n",
        "                                                           max_nodes=4, \r\n",
        "                                                           vm_priority=\"lowpriority\",\r\n",
        "                                                           idle_seconds_before_scaledown=2400)\r\n",
        "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\r\n",
        "\r\n",
        "cpu_cluster.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing cluster, use it.\n",
            "Succeeded\n",
            "AmlCompute wait for completion finished\n",
            "\n",
            "Minimum number of nodes requested have been provisioned\n"
          ]
        }
      ],
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1614123251796
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Environment "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure AML Curated Environemnt and custom JAR packages with a pip environment that has `scikit-learn`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.environment import Environment\n",
        "from azureml.core import RunConfiguration\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "\n",
        "spark_env=Environment.get(workspace=ws, name=\"AzureML-PySpark-MmlSpark-0.15\")\n",
        "spark_env = spark_env.clone(\"PySpark-MmlSpark-Alt\")\n",
        "\n",
        "# Add \n",
        "conda_dep = CondaDependencies()\n",
        "\n",
        "# Installs scikit-learn pip package\n",
        "conda_dep.add_pip_package(\"scikit-learn\")\n",
        "\n",
        "# Adds dependencies to PythonSection of myenv\n",
        "spark_env.python.conda_dependencies=conda_dep\n",
        "spark_env.spark.packages = [{\"group\": \"com.databricks\",\"artifact\": \"spark-xml_2.11\",\"version\": \"0.6.0\"}]\n",
        "spark_env.spark.repositories = [\"https://mvnrepository.com/artifact/com.databricks/spark-xml\"]"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1614123252130
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure AML Cluster "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the Framework as PySpark, set cluster name and environment the AML Cluster will use"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use pyspark framework\r\n",
        "spark_run_config = RunConfiguration(framework=\"PySpark\")\r\n",
        "\r\n",
        "# Set compute target to the cpu cluster\r\n",
        "spark_run_config.target = cpu_cluster.name\r\n",
        "\r\n",
        "# Set node count for Spark job\r\n",
        "spark_run_config.node_count = 1\r\n",
        "\r\n",
        "# Set environment\r\n",
        "spark_run_config.environment = spark_env"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1614135034913
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Datastore and DataReference for Spark to talk to"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO-DO, write instruction on how to mount to a Blob datastore"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Datastore\r\n",
        "\r\n",
        "stackoverflow_datastore = ws.datastores['stackoverflow_blob']\r\n",
        "data_ref = stackoverflow_datastore.as_mount()"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1614123253004
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Submit the script to AzureML Compute"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import ScriptRunConfig, Environment\n",
        "\n",
        "script_run_config = ScriptRunConfig(source_directory = '.',\n",
        "                                    script= 'stackoverflow-data-prep.py',\n",
        "                                    arguments=[str(data_ref)],\n",
        "                                    run_config = spark_run_config,\n",
        "                                    )\n",
        "script_run_config.run_config.data_references[data_ref.data_reference_name] = data_ref.to_config()\n",
        "run = exp.submit(config=script_run_config)\n"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1614135046951
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monitor the run using a Juypter widget"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "RunDetails(run).show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ca611b9fabd407fa922f18848d658c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/data-processing-on-spark/runs/data-processing-on-spark_1614129782_d4bb7141?wsid=/subscriptions/e9b2ec51-5c94-4fa8-809a-dc1e695e4896/resourcegroups/zhenzhuuksouth/workspaces/zhenzhuuksouth\", \"run_id\": \"data-processing-on-spark_1614129782_d4bb7141\", \"run_properties\": {\"run_id\": \"data-processing-on-spark_1614129782_d4bb7141\", \"created_utc\": \"2021-02-24T01:23:06.519194Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"bd7334ab-9b48-4b91-83b4-ab77d88f30c4\", \"azureml.git.repository_uri\": \"git@github.com:AbeOmor/Spark-in-AzureML.git\", \"mlflow.source.git.repoURL\": \"git@github.com:AbeOmor/Spark-in-AzureML.git\", \"azureml.git.branch\": \"main\", \"mlflow.source.git.branch\": \"main\", \"azureml.git.commit\": \"66e058f865c4d161cd2b4ab78b235f9395df1a06\", \"mlflow.source.git.commit\": \"66e058f865c4d161cd2b4ab78b235f9395df1a06\", \"azureml.git.dirty\": \"True\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {\"_aml_system_ComputeTargetStatus\": \"{\\\"AllocationState\\\":\\\"resizing\\\",\\\"PreparingNodeCount\\\":0,\\\"RunningNodeCount\\\":0,\\\"CurrentNodeCount\\\":2}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2021-02-24T01:31:20.459906Z\", \"status\": \"Failed\", \"log_files\": {\"azureml-logs/55_azureml-execution-tvmps_47a711daf1c1765f6f4d0554681237fc21a4425f1951099844619f4120b3e628_p.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614129782_d4bb7141/azureml-logs/55_azureml-execution-tvmps_47a711daf1c1765f6f4d0554681237fc21a4425f1951099844619f4120b3e628_p.txt?sv=2019-02-02&sr=b&sig=hubkv5UP9c47tD%2BKhjaPKyBq1vzTJRHfLQzvwvPJ%2B%2Fc%3D&st=2021-02-24T01%3A21%3A15Z&se=2021-02-24T09%3A31%3A15Z&sp=r\", \"azureml-logs/55_azureml-execution-tvmps_7477010d0c1a2d36f6d03008d9e0b6bcb36c29ff34b4669fbe10536dd703e576_p.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614129782_d4bb7141/azureml-logs/55_azureml-execution-tvmps_7477010d0c1a2d36f6d03008d9e0b6bcb36c29ff34b4669fbe10536dd703e576_p.txt?sv=2019-02-02&sr=b&sig=RrhZfeeRajaffCfDQ8UOuxDHgNOH%2FVFn0LNd3%2Bplovo%3D&st=2021-02-24T01%3A21%3A15Z&se=2021-02-24T09%3A31%3A15Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_47a711daf1c1765f6f4d0554681237fc21a4425f1951099844619f4120b3e628_p.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614129782_d4bb7141/azureml-logs/65_job_prep-tvmps_47a711daf1c1765f6f4d0554681237fc21a4425f1951099844619f4120b3e628_p.txt?sv=2019-02-02&sr=b&sig=H%2FAXHejfqyuHU5iWDpy7M7ciV2FlTaagI%2BvDWD0YTm4%3D&st=2021-02-24T01%3A21%3A15Z&se=2021-02-24T09%3A31%3A15Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_7477010d0c1a2d36f6d03008d9e0b6bcb36c29ff34b4669fbe10536dd703e576_p.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614129782_d4bb7141/azureml-logs/65_job_prep-tvmps_7477010d0c1a2d36f6d03008d9e0b6bcb36c29ff34b4669fbe10536dd703e576_p.txt?sv=2019-02-02&sr=b&sig=Od2SjOg6le%2Fo8XKkaMHzmtYkpHkjIkAkoC4DjCkat%2FQ%3D&st=2021-02-24T01%3A21%3A15Z&se=2021-02-24T09%3A31%3A15Z&sp=r\", \"azureml-logs/70_driver_log_0.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614129782_d4bb7141/azureml-logs/70_driver_log_0.txt?sv=2019-02-02&sr=b&sig=Xvi0%2BgRv5%2BqppY7q%2FzsglqtP2En5wiWt0vvOCm6SpWs%3D&st=2021-02-24T01%3A21%3A15Z&se=2021-02-24T09%3A31%3A15Z&sp=r\", \"azureml-logs/70_driver_log_1.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614129782_d4bb7141/azureml-logs/70_driver_log_1.txt?sv=2019-02-02&sr=b&sig=kZ7MwwvcvnMkM128khhsGkG3PySOPjqyP5nObwcAaJo%3D&st=2021-02-24T01%3A21%3A15Z&se=2021-02-24T09%3A31%3A15Z&sp=r\", \"azureml-logs/70_mpi_log.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614129782_d4bb7141/azureml-logs/70_mpi_log.txt?sv=2019-02-02&sr=b&sig=I54kb%2B0GbmN5cpkCygiS25WBVoz%2BqJT7yCadVVMesJg%3D&st=2021-02-24T01%3A21%3A15Z&se=2021-02-24T09%3A31%3A15Z&sp=r\", \"azureml-logs/75_job_post-tvmps_47a711daf1c1765f6f4d0554681237fc21a4425f1951099844619f4120b3e628_p.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614129782_d4bb7141/azureml-logs/75_job_post-tvmps_47a711daf1c1765f6f4d0554681237fc21a4425f1951099844619f4120b3e628_p.txt?sv=2019-02-02&sr=b&sig=xodegoDxkrF73JnEnyYYFyYdDF2BW33gGCjPDe%2BbZRg%3D&st=2021-02-24T01%3A21%3A15Z&se=2021-02-24T09%3A31%3A15Z&sp=r\", \"azureml-logs/75_job_post-tvmps_7477010d0c1a2d36f6d03008d9e0b6bcb36c29ff34b4669fbe10536dd703e576_p.txt\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614129782_d4bb7141/azureml-logs/75_job_post-tvmps_7477010d0c1a2d36f6d03008d9e0b6bcb36c29ff34b4669fbe10536dd703e576_p.txt?sv=2019-02-02&sr=b&sig=LQnG2%2BrKYZ%2BE0p9CUiQHUOwFTQC%2FEI5egnhAsHGtcws%3D&st=2021-02-24T01%3A21%3A15Z&se=2021-02-24T09%3A31%3A15Z&sp=r\", \"azureml-logs/process_info.json\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614129782_d4bb7141/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=mu5Ve3jUnHfAr2KUHARfWR9ToM9zgnD5XbBTvJMkiZo%3D&st=2021-02-24T01%3A21%3A15Z&se=2021-02-24T09%3A31%3A15Z&sp=r\", \"azureml-logs/process_status.json\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614129782_d4bb7141/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=qLKiwT9cu0b0r3K%2BsH3kljA7IRwd6zjsnmu%2BKkmOpxU%3D&st=2021-02-24T01%3A21%3A15Z&se=2021-02-24T09%3A31%3A15Z&sp=r\", \"logs/azureml/0_546_azureml.log\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614129782_d4bb7141/logs/azureml/0_546_azureml.log?sv=2019-02-02&sr=b&sig=lI9J3HUlqQ3cZRq0j1%2BxtKbZUtQQao6v8fSHirtBuRY%3D&st=2021-02-24T01%3A21%3A15Z&se=2021-02-24T09%3A31%3A15Z&sp=r\", \"logs/azureml/1_597_azureml.log\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614129782_d4bb7141/logs/azureml/1_597_azureml.log?sv=2019-02-02&sr=b&sig=qcpG7mrXJ0SdLPcWqgcGv1loY4Cmwn0NnSKwEdkXjl8%3D&st=2021-02-24T01%3A21%3A15Z&se=2021-02-24T09%3A31%3A15Z&sp=r\", \"logs/azureml/job_prep_azureml.log\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614129782_d4bb7141/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=laP%2Fjrt1M3Y9hqEcnwzx13laKNpDVn9WD8penYkOyeo%3D&st=2021-02-24T01%3A21%3A15Z&se=2021-02-24T09%3A31%3A15Z&sp=r\", \"logs/azureml/job_release_azureml.log\": \"https://zhenzhuuksouth3632161177.blob.core.windows.net/azureml/ExperimentRun/dcid.data-processing-on-spark_1614129782_d4bb7141/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=jTY1MAPC0M9vNcOWp9znevfCN8EO4N0pfZl6rruDbp8%3D&st=2021-02-24T01%3A21%3A15Z&se=2021-02-24T09%3A31%3A15Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\", \"logs/azureml/job_prep_azureml.log\", \"logs/azureml/job_release_azureml.log\"], [\"logs/azureml/0_546_azureml.log\"], [\"logs/azureml/1_597_azureml.log\"], [\"azureml-logs/55_azureml-execution-tvmps_47a711daf1c1765f6f4d0554681237fc21a4425f1951099844619f4120b3e628_p.txt\", \"azureml-logs/55_azureml-execution-tvmps_7477010d0c1a2d36f6d03008d9e0b6bcb36c29ff34b4669fbe10536dd703e576_p.txt\"], [\"azureml-logs/65_job_prep-tvmps_47a711daf1c1765f6f4d0554681237fc21a4425f1951099844619f4120b3e628_p.txt\", \"azureml-logs/65_job_prep-tvmps_7477010d0c1a2d36f6d03008d9e0b6bcb36c29ff34b4669fbe10536dd703e576_p.txt\"], [\"azureml-logs/70_mpi_log.txt\", \"azureml-logs/70_driver_log_0.txt\", \"azureml-logs/70_driver_log_1.txt\"], [\"azureml-logs/75_job_post-tvmps_47a711daf1c1765f6f4d0554681237fc21a4425f1951099844619f4120b3e628_p.txt\", \"azureml-logs/75_job_post-tvmps_7477010d0c1a2d36f6d03008d9e0b6bcb36c29ff34b4669fbe10536dd703e576_p.txt\"]], \"run_duration\": \"0:08:13\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"[2021-02-24T01:31:07.403057] Entering job release\\r\\n[2021-02-24T01:31:08.331126] job release stage : copy_batchai_cached_logs starting...\\r\\n[2021-02-24T01:31:08.331172] job release stage : copy_batchai_cached_logs completed...\\r\\n\\nError occurred: AzureMLCompute job failed.\\nJobFailed: Submitted script failed with a non-zero exit code; see the driver log file for details.\\n\\tReason: Job failed with non-zero exit Code\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.22.0\"}, \"loading\": false}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1614129789322
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: if you need to cancel a run, you can follow [these instructions](https://aka.ms/aml-docs-cancel-run)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the run is succesfully finished, you can check the metrics logged."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "index_order": 1,
    "exclude_from_index": false,
    "task": "Submiting a run on a spark cluster",
    "deployment": [
      "None"
    ],
    "authors": [
      {
        "name": "sanpil"
      }
    ],
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "compute": [
      "HDI cluster"
    ],
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "tags": [
      "None"
    ],
    "datasets": [
      "None"
    ],
    "categories": [
      "how-to-use-azureml",
      "training"
    ],
    "category": "training",
    "framework": [
      "PySpark"
    ],
    "friendly_name": "Training in Spark",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}